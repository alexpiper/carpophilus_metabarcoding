---
title: "carpophilus_metabarcoding"
author: "Alexander Piper"
date: "28/08/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knitr global setup - change eval to true to run code

library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE,fig.show = "hold", fig.keep = "all")
opts_knit$set(root.dir = 'C:/Users/ap0y/Dropbox/workprojects/PHD/Metabarcoding/Dros_metabarcoding')
setwd('C:/Users/ap0y/Dropbox/workprojects/PHD/Metabarcoding/Dros_metabarcoding')
opts_chunk$set(dev = 'png')
```

# Introduction 

# Demultiplex samples using illumina indexes

BASH:
```{bash demultiplex 1 mismatch}
###BASH###

#raise amount of available file handles
ulimit -n 4000

#Miseq Run 1 - Testing Primers
/home/ap0y/usr/local/bin/bcl2fastq -p 12 --runfolder-dir /group/sequencing/190331_M03633_0310_000000000-CB3DR  --output-dir /group/pathogens/Alexp/Metabarcoding/Run1_primertest --sample-sheet /group/pathogens/Alexp/Metabarcoding/Run1_primertest/SampleSheet_primertest.csv --no-lane-splitting --barcode-mismatches 0


#Miseq Run 2 - Testing replicate tagged primers & SynMock
/home/ap0y/usr/local/bin/bcl2fastq -p 12 --runfolder-dir /group/sequencing/190628_M03633_0331_000000000-CK3HD  --output-dir /group/pathogens/Alexp/Metabarcoding/Run2_reptest --sample-sheet /group/pathogens/Alexp/Metabarcoding/Run2_reptest/SampleSheet_reptest.csv --no-lane-splitting --barcode-mismatches 0

#Miseq Run 3 - Ladder spike ins
/home/ap0y/usr/local/bin/bcl2fastq -p 12 --runfolder-dir /group/sequencing/190722_M03633_0336_000000000-CJKFJ/  --output-dir /group/pathogens/Alexp/Metabarcoding/Run3_spikein --sample-sheet /group/pathogens/Alexp/Metabarcoding/Run3_spikein/SampleSheet_run3.csv --no-lane-splitting --barcode-mismatches 0
```


#Implement savr quality chekc

https://www.bioconductor.org/packages/release/bioc/vignettes/savR/inst/doc/savR.pdf

# Calculate index switch rate

While using unique dual-indices will allow detection and removal of the majority of index switch reads, there will still be low level undetectable index switching present at a rate of obs/exp^2 (ref- ). to determine this rate, we will first calculate the unexpected index combinations compared to the expected. 

Fastq files contain the index information for each read in the read header, and therefore to get all undetermined indices, both switched and otherwise erroneous we can summarise the index sequences for each read as contained in the fasta header:

@M03633:307:000000000-D4262:1:1101:19524:28535 1:N:0:**GAGACGAT+GTTCTCGT**
ATACTGTGCGTACTGCAGATCGGAAGAGCACACGTCTGAACTCCAGTCACGAGACGATATCTCGTATGCCGTCTT 
+ 
BBBB?FFFBABAEGGGGGGGGGGGGGFHHHHHHGHHHGHHHHHHHHHHHHGGEEEEEAFGGHGHFAHHHHGGGH


First we need to demultiplex the data again allowing no mismatches - This needs to be done with bcl2fastq rather than the default illumina miseq lociratefastq workflow, as the workflow doesnt include indices in fastq file headers

## Install and load packages
```{r install & Load packages} 
#Set required packages
.cran_packages <- c("ggplot2", "gridExtra","tidyverse","scales","stringdist","patchwork","vegan","ggpubr","seqinr","viridis")
.bioc_packages <- c("dada2", "phyloseq", "DECIPHER","Biostrings","ShortRead","psadd","ggridges")
#.github_packages <- c("metacal", "taxreturn", "piperline")

#.inst <- .cran_packages %in% installed.packages()
#if(any(!.inst)) {
#   install.packages(.cran_packages[!.inst])
#}
#.inst <- .bioc_packages %in% installed.packages()
#if(any(!.inst)) {
#  if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
#  BiocManager::install(.bioc_packages[!.inst], ask = F)
#}
#
#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

#Load helper functions 
library(taxreturn)
library(speedyseq)
library(metacal)

#devtools::install_github("benjjneb/dada2")

source("scripts/helper_functions.R")

```



Then we can summarise the switch rate by counting unassigned reads

```{bash undetermined}
###BASH###
#Run 1
#summarise undetermined reads from R1 undetermined file
zcat Undetermined_S0_R1_001.fastq.gz | grep '^@M03633' | cut -d : -f 10 | sort | uniq -c | sort -nr > undetermined.txt

# summarise correctly determined reads from all other R1 files 
rm determined.txt
ls | grep "R1_001.fastq.gz" | sort | grep -v 'Undetermined' > test_ls_F

let files=$(grep -c "fastq.gz" test_ls_F)

declare -i x

x=1
while [ $x -le $files ] 
    do

query=$(sed -n "${x}p" test_ls_F)

sample_name=$(echo $query | awk -F . '{ print $1}')

stats=$(zcat $(echo $query) | grep '^@M03633' | wc -l)
echo $query $stats >> determined.txt

let x=x+1

done 

```

To differentiate unused indices arising from switching, from unused indices arrising from other phenomena, we can compare the undetermined count file to all possible combinations of i5 and i7 indices that could be produced through switching 

```{r index switching, eval=TRUE}
#Read in original sample sheet
SampleSheet <- read_csv("Run3_spikein/SampleSheet_run3.csv",skip=20)

##For special case of popgen spikein
SampleSheet <- read_csv("Run3_spikein/SampleSheet_run3.csv",skip=20)  %>%
  filter(str_detect(Sample_Name,pattern="-"))

##enumerate 1 mismatch to all indices to mimic demultiplexing with a single mismatch

I7_Index_ID <- SampleSheet$index
I5_Index_ID  <- SampleSheet$index2

#Create all possible switched combinations
combos <- unique(expand.grid(I7_Index_ID, I5_Index_ID))
combos$indices <- paste0(combos$Var1,"+",combos$Var2)

#Determined reads from mock communities
determined <- read_table2("Run3_spikein/determined.txt",col_names = FALSE)
colnames(determined) <- c("Sample_Name","count")
determined$Sample_Name <- determined$Sample_Name %>%
  str_split_fixed("_S",n=2)
determined$Sample_Name <- determined$Sample_Name[,1]
determined <- left_join(determined, SampleSheet, by="Sample_Name") 
determined$indices <- paste0(determined$index, "+",determined$index2)
determined <- determined %>%
  subset(select=c("Sample_Name","count","indices"))
head(determined)

#Undetermined reads from mock communities
undetermined <- read_table("Run3_spikein/undetermined.txt",col_names = FALSE)
colnames(undetermined) <- c("count","indices")
undetermined$Sample_Name <- "Undetermined_S0_R1_001.fastq.gz"
head(undetermined)

indices <- rbind(determined,undetermined)

#Calculate total read count for run
total_reads <- sum(indices$count)

#get unused combinations resulting from index switching
switched <- left_join(combos,indices,by="indices") %>%
  drop_na()
colnames(switched) <- c("i7","i5","indices","Sample_Name","count")

#get unused combinations resulting from other phenomena
other <- indices[!indices$indices %in% combos$indices, ]

#Count number of other undetermined
other_reads <- sum(other$count)

##Summary of index switching rate
exp_rate <- switched %>% 
  filter(!str_detect(Sample_Name,"Undetermined"))
obs_rate <- switched %>% 
  filter(str_detect(Sample_Name,"Undetermined"))

switch_rate <- (sum(obs_rate$count)/sum(exp_rate$count))
message(switch_rate)

#Rate of undetected switching should be switch_rate squared
filt_threshold <- switch_rate^2
message(paste0("The threshold for filtering will be: ",filt_threshold))

#Plot switching

gg.switch <- ggplot(data = switched, aes(x = i7, y = i5), stat="identity") +
  geom_tile(aes(fill = count),alpha=0.8)  + scale_fill_viridis(name = "reads", begin=0.1,trans = "log10")  + 
  theme(axis.text.x = element_text(angle=90, hjust=1), plot.title=element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5), legend.position = "none") +
  labs(title= "Run3_spikeins", subtitle = paste0("Total Reads: ", total_reads, " Switch rate: ", sprintf("%1.2f%%", switch_rate*100)))


#Plot pooling

gg.pooling <- ggplot(data=determined, aes(x=fct_rev(Sample_Name),y=count),stat="identity") + 
  geom_bar(aes(fill=count),stat="identity")  + 
  scale_fill_viridis(name = "reads", begin=0.1) + 
  theme(axis.text.x = element_text(angle=90, hjust=1), plot.title=element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5))+ 
  geom_hline(aes(yintercept = mean(count)))  +
  xlab("Sample")+
  ylab("number of reads") +
  labs(title= "Pooling", subtitle = paste0("Total Reads: ", total_reads, " Average reads: ",  sprintf("%.0f",mean(determined$count))," Standard deviation: ", sprintf("%.0f",sd(determined$count)))) +
  coord_flip()


##Special case - plot pooling of popgen spiked

popgen <- determined %>%
  filter(!str_detect(Sample_Name,pattern="-"))

gg.popgen <- ggplot(data=popgen, aes(x=fct_rev(Sample_Name),y=count),stat="identity") + 
  geom_bar(aes(fill=count),stat="identity")  + 
  scale_fill_viridis(name = "reads", begin=0.1) + 
  theme(axis.text.x = element_text(angle=90, hjust=1), plot.title=element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5))+ 
  geom_hline(aes(yintercept = mean(count)))  +
  xlab("Synthetic sample")+
  ylab("number of reads") +
  #labs(title= "Pooling", subtitle = paste0("Total Reads: ", total_reads, " Average reads: ",  #sprintf("%.0f",mean(determined$count))," Standard deviation: ", sprintf("%.0f",sd(determined$count)))) +
  coord_flip()


```


# Demultiplex by primer & trim

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Prior to begining this workflow, samples were demultiplexed and illumina adapters were removed by the MiSeq software, however primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm.

In this study there were 2 amplicons of different size, and 3 different replicate primers of each. For this workflow we will be using the Kmer based adapter trimming software BBDuk (Part of BBTools package https://jgi.doe.gov/data-and-tools/bbtools/) to trim the primers from our raw data files.

## fwhF2-fwhR2n amplicon:

FORWARD PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhF2T1_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	  GAGGDACWGGWTGAACWGTWTAYCCHCC
    fwhF2T2_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	 TGTGGDACWGGWTGAACWGTWTAYCCHCC
    fwhF2T3_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	AGAAGGDACWGGWTGAACWGTWTAYCCHCC
    
REVERSE PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhR2nT1_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	ACGTRATWGCHCCDGCTARWACWGG
    fwhR2nT2_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	TCCGTRATWGCHCCDGCTARWACWGG
    fwhR2nT3_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	CTGCGTRATWGCHCCDGCTARWACWGG
    
```{r Demultiplex,message=FALSE}
#Install bbmap
bbmap_install()

#Demultiplex samples

runs <- dir("data/", pattern="run3")
path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files

demuxpath <- file.path(path, "demux") # Filtered forward files go into the path/filtered/ subdirectory
dir.create(demuxpath)

fastqFs <- sort(list.files(path, pattern="R1_001.*", full.names = TRUE))
fastqRs <- sort(list.files(path, pattern="R2_001.*", full.names = TRUE))


bbtools_demux(install="bin/bbmap", fwds=fastqFs, revs=fastqRs,Fbarcodes = c("GAGGDACW","TGTGGDAC","AGAAGGDAC"),
              Rbarcodes = c("ACGTRATW","TCCGTRAT","CTGCGTRA"),copyundefined=TRUE, outpath=demuxpath,hdist=0, overwrite=TRUE)

demux_fastqs <- sort(list.files(paste0(demuxpath), pattern="_R1R2_", full.names = TRUE))


bbtools_trim(install="bin/bbmap", fwd=demux_fastqs,primers=c("GGDACWGGWTGAACWGTWTAYCCHCC","GTRATWGCHCCDGCTARWACWGG"), copyundefined=TRUE, outpath="trimmed",ktrim="l", ordered=TRUE,mink=FALSE, hdist=2,maxlength=140, overwrite=TRUE)

#Re-split interleaved fastq's

trimmedpath <- file.path(demuxpath, "trimmed") # Filtered forward files go into the path/filtered/ subdirectory
trimmed_fastqs <- sort(list.files(trimmedpath, pattern="_R1R2_", full.names = TRUE))

bbtools_split(install="bin/bbmap",files=trimmed_fastqs, overwrite=TRUE)


#test deal with qualities

#aqhist.txt <- average read quality
#qhist.txt <- read quality across length
#bhist <- base pair composition
#gchist <- read gc content
#lhist <- length histogram

qual <- read_table2("qhist.txt")
#Plot read 1 qualities
plot(qual$Read1_linear)
#plot read 2 qualities
plot(qual$fraction2)


```


## Error visualisation

We start by visualizing the quality profiles of the forward read and reverse reads for each run:


## Sequence quality control

Options here - if using LINUX/MAC do fastqcr - If using windows use shortrread

```{r }
library(ShortRead)

runs <- dir("data/", pattern="run3")

for (i in seq(along=runs)){
path <- paste0("data/",runs[i],"/demux/trimmed/") # CHANGE ME to the directory containing your demultiplexed forward-read fastq files

  #path <- paste0("data/",runs[i])

#Plot number of reads
dat <- as.data.frame(countLines(dirPath=path, pattern=".fastq")) %>%
  rownames_to_column()  %>%
   `colnames<-`(c("Sample", "Reads")) %>%
  filter(str_detect(Sample,"R1"))

#Plot pooling

gg.pooling <- ggplot(data=dat, aes(x=Sample,y=Reads),stat="identity") + 
  geom_bar(aes(fill=Reads),stat="identity")  + 
  scale_fill_viridis(name = "Reads", begin=0.1) + 
  theme(axis.text.x = element_text(angle=90, hjust=1), plot.title=element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5))+ 
  geom_hline(aes(yintercept = mean(Reads)))  +
  xlab("sample name")+
  ylab("Number of reads") + 
  labs(title= "Pooling", subtitle = paste0("Total Reads: ", sum(dat$Reads), " Average reads: ",  sprintf("%.0f",mean(dat$Reads))," Standard deviation: ", sprintf("%.0f",sd(dat$Reads)))) +
  coord_flip()

# Put in a check for operating system here 
sys <- Sys.info()[['sysname']]

#if (sys == "Mac") send to fastqcr 
#if (sys == "Windows") send to Shortread
  
# Plot quality stat using shortread

#fls <- dir(path, "fastq.gz", full=TRUE)

fastqFs <- sort(list.files(path, pattern="R1_001.*", full.names = TRUE))
fastqRs <- sort(list.files(path, pattern="R2_001.*", full.names = TRUE))

fls <- fastqRs[30:40]

coll <- QACollate(QAFastqSource(fls), QAReadQuality(),
    QAAdapterContamination(), QANucleotideUse(),
    QAQualityUse(), QASequenceUse(),
    QAFrequentSequence(n=10), QANucleotideByCycle(),
    QAQualityByCycle())
x <- qa2(coll, BPPARAM=SerialParam(), verbose=TRUE)

res <- report(x)
if (interactive()) browseURL(res)  


#fastqFs <- sort(list.files(path, pattern="R1_001.*", full.names = TRUE))
#fastqRs <- sort(list.files(path, pattern="R2_001.*", full.names = TRUE))
# p1 <- plotQualityProfile(fastqFs[50:60]) + ggtitle(paste0(runs[i]," Forward Reads"))
#  p2 <- plotQualityProfile(fastqRs[1:4]) + ggtitle(paste0(runs[i]," Reverse Reads"))
#  print(p1+p2)
}
```


In gray-scale is a heat map of the frequency of each quality score at each base position. The median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same lenghth, hence the flat red line).

The forward reads are good quality. We generally advise trimming the last few nucleotides to avoid less well-controlled errors that can arise there. These quality profiles do not suggest that any additional trimming is needed. We will truncate the forward reads at position 240 (trimming the last 10 nucleotides).

The reverse reads are of significantly worse quality, especially at the end, which is common in Illumina sequencing. This isn’t too worrisome, as DADA2 incorporates quality information into its error model which makes the algorithm robust to lower quality sequence, but trimming as the average qualities crash will improve the algorithm’s sensitivity to rare sequence variants. Based on these profiles, we will truncate the reverse reads at position 160 where the quality distribution crashes.

## Filter and trim

The forward reads for the hemiptera metabarcoding data are of good quality, while The reverse reads are of slightly worse worse quality at the end, which is common in Illumina sequencing. Informed by these profiles, we will use the Truncate quality function (TruncQ=2) to cut the reads at any point the Q score crashes below 2.

the maxEE should be used as the primary quality filter. TruncQ is mostly to remove very low quality sequences

In general pick truncLen parameters that avoid the worst parts of the quality profiles but ensure that enough sequence is kept to healthily overlap (truncLen[[1]] + truncLen[[2]] > amplicon_length+25), leave truncQ=2, and try a couple maxEE values until I get a satisfactory number of reads through the filter


```{r filter and trim}
##Note - for filtering stage, these parameters may not be optimal for each run or testset, use the previous plotting step to inform this
runs <- dir("data/", pattern="run3")
filtered_out <- list()

for (i in seq(along=runs)){
  path <- paste0("data/",runs[i],"/demux/trimmed/") # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  dir.create(filtpath)
  fastqFs <- sort(list.files(path, pattern="R1_001.*"))
  fastqRs <- sort(list.files(path, pattern="R2_001.*"))
  
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))
  
  filtered_out[[i]] <- (filterAndTrim(fwd=file.path(path, fastqFs), filt=file.path(filtpath, fastqFs),
                                      rev=file.path(path, fastqRs), filt.rev=file.path(filtpath, fastqRs),
                                      maxEE=2, truncQ=15, maxN = 0,trimLeft = c(51,0), truncLen = 125,
                                      rm.phix=TRUE, compress=TRUE, verbose=TRUE))
}
print(filtered_out)

```


## Post filtering error plotting

sanity check to see the effects of the filter and trim step

```{r Post filter plot, eval = TRUE, cache= TRUE}
runs <- dir("data/", pattern="run3")

##Post filtering plotting
for (i in seq(along=runs)){
 path <- paste0("data/",runs[i],"/demux/trimmed/" )# CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  
  filtFs <- sort(list.files(filtpath, pattern="R1_001.*", full.names = TRUE))
  filtRs <- sort(list.files(filtpath, pattern="R2_001.*", full.names = TRUE))
  p1 <- plotQualityProfile(filtFs[1:20]) + ggtitle(paste0(runs[i]," Filtered Forward Reads"))
  p2 <- plotQualityProfile(filtRs[1:20]) + ggtitle(paste0(runs[i]," Filtered Reverse Reads"))
  print(p1+p2)
}

```


## Infer sequence variants

Every amplicon dataset has a different set of error rates and the DADA2 algorithm makes use of a parametric error model (err) to model this and infer real biological sequence variation from error. Following error model learning, all identical sequencing reads are dereplicated into into “Exact sequence variants” with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.

For this analysis we will use all the reads to estimate error rate, and plot the error model for each run as a sanity check

Is it worth playing with to increase sensitivity for target pests here?
https://benjjneb.github.io/dada2/pseudo.html#Pseudo-pooling

The purpose of priors is to increase sensitivity to a restricted set of sequences, including singleton detection, without increasing false-positives from the unrestricted set of all possible amplicon sequences that must be considered by the naive algorithm

```{r Learn error rates }
runs <- dir("data/", pattern="run3")
set.seed(100)

for (i in seq(along=runs)){
 path <- paste0("data/",runs[i],"/demux/trimmed/" )# CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  
  filtFs <- list.files(filtpath, pattern="R1_001.*", full.names = TRUE)
  filtRs <- list.files(filtpath, pattern="R2_001.*", full.names = TRUE)
  
    # Learn error rates from samples
  # nread tells the function how many reads to use in error learning, this can be increased for more accuracy at the expense of runtime
  
  errF <- learnErrors(filtFs, multithread=TRUE, randomize=TRUE)
  errR <- learnErrors(filtRs, multithread=TRUE, randomize=TRUE)
  
  ##Print error plots to see how well the algorithm modelled the errors in the different runs
  print(plotErrors(errF, nominalQ=TRUE)+ ggtitle(paste0(runs[i]," Forward Reads")))
  print(plotErrors(errR, nominalQ=TRUE)+ ggtitle(paste0(runs[i]," Reverse Reads")))
  
  #Error inference and merger of reads - Using pseudo pooling for increased sensitivity

  dadaFs <- dada(filtFs, err=errF, multithread=TRUE, pool="pseudo")
  dadaRs <- dada(filtRs, err=errR, multithread=TRUE, pool="pseudo")
 
  mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
  
# Construct sequence table

seqtab <- makeSequenceTable(mergers)

write_rds(seqtab, paste0(path,"/seqtab.rds")) # CHANGE ME to where you want sequence table saved
}
```

Check for length variation by primer

## Merge Runs, Remove Chimeras

Now that the sequence tables are created for each run, they need to be merged into a larger table representing the entire study. Following this, chimeric sequences are identified and removed using removeBimeraDenovo, and any identical sequences with the only difference being length variation are collapsed using collapseNoMismatch.

```{r merge runs and remove chimeras}
runs <- dir("data/", pattern="run")
stlist <- vector()

for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  seqs <- list.files(path, pattern="seqtab.rds", full.names = TRUE)
  
  assign(paste("st", i, sep = ""),read_rds(seqs))
  stlist <- append(stlist, paste("st", i, sep = ""), after=length(seqs))
}

st.all <- mergeSequenceTables(st1, st2, st3)

#Collapse all sequences together when the only difference between them is length variation - This is unnecessary for single PCR markers, however as we are using 2 pcr amplicons with slightly different lengths this will simplify analysis
#st.all <- st1

st.all <- collapseNoMismatch(st.all, minOverlap = 20, orderBy = "abundance",
                                   vec = TRUE, verbose = TRUE)

#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=FALSE, verbose=TRUE)

#Check output of chimera removal
print(paste(sum(seqtab.nochim)/sum(st.all),"of the abundance remaining after chimera removal"))

#Check complexity

hist(seqComplexity(seqtab.nochim), 100)

#Remove sequences of wrong size
#seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 250:256])


#Fix names -removing read name, sample number etcc
rownames(seqtab.nochim) <- rownames(seqtab.nochim) %>% 
  str_replace_all(pattern="\\.rep",replacement="_Rep") %>% 
  str_split_fixed("_",n=Inf) %>%
  as_tibble() %>%
  mutate(V5 = V5 %>% str_split_fixed("\\.",n=2) %>% as_tibble() %>% pull(V1))%>%
  unite(V1,c(V1,V5),sep="_") %>%
  pull(V1) %>%
  str_replace(pattern="_$",replacement="") # Drop trailing underscore

dir.create("output/rds/")
write_rds(seqtab.nochim, "output/rds/seqtab_final_Run3.rds") # CHANGE ME to where you want sequence table saved

```

## Assign taxonomy with IDTAXA & Exact matching

We will use the IDTAXA algorithm of Murali et al 2018 - https://doi.org/10.1186/s40168-018-0521-5

This requires training on a curated reference database - The pre-trained file can be found in the reference folder, alternatively see the taxreturn scripts to curate a reference database and train a new classifier.

Folllowing assignment with IDTAXA, we will also use exact matching with a reference database to assign more sequences (including the synthetic positive controls) to species level

```{r IDTAXA}

seqtab.nochim <- read_rds("output/rds/seqtab_final.rds")

trainingSet <- read_rds("reference/merged_arthropoda_idtaxa.rds")
dna <- DNAStringSet(getSequences(seqtab.nochim)) # Create a DNAStringSet from the ASVs

##Decide on threshold
ids <- IdTaxa(dna, trainingSet, processors=1,threshold = 60, verbose=TRUE)  #WARNING - assigning more than one processor currently crashes R

#test <- ids
plot(ids, trainingSet)

#ids <- test

#delete existing file
cat("",file="idtaxa.csv")
for (i in 1:length(ids)){
 lines <- as.data.frame(t(cbind(ids[[i]]$taxon,ids[[i]]$confidence)))
 rownames(lines) <- c("taxa","confidence")
write.table(lines,file="idtaxa.csv",sep=",",append=TRUE, col.names=FALSE)
}

ranks <-  c("Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") # ranks of interest
#Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
tax <- t(sapply(ids, function(x) {
        taxa <- x$taxon
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))

library(stringi)
tax <- stri_list2matrix(lapply(tax, unlist), byrow=TRUE, fill=NA)

#Add sequences and column names to matrix
colnames(tax) <- ranks; rownames(tax) <- getSequences(seqtab.nochim)

#Subset to remove the root rank
tax <- subset(tax, select=c("Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species"))

#Propagate high order ranks to unassigned ASV's
tax <- propagate_tax(tax,from="Phylum") 


#Check Output
taxa.print <- tax # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

# Write taxonomy table to disk
write_rds(tax, "output/rds/tax_IdTaxa.rds") 
tax <- read_rds("output/rds/tax_IdTaxa.rds") 

#Add missed species using exact matching

#
exact <- assignSpecies(seqtab.nochim, "reference/merged_rdp_species_synsadded.fa.gz", allowMultiple = TRUE, tryRC = TRUE, verbose = FALSE)

exact <- read_rds("output/rds/exact.rds")

exact <- exact %>% 
  as_tibble() %>%
  mutate(binomial =  case_when(!is.na(Species) ~  paste0(Genus,"_",Species)))

#merge together
#For exact where Species is not NA, replace tax$Species where Species contains K__,P__,C__,O__,F__,G__
pattern <- c("K__","P__","C__","O__","F__","G__")
for (row in 1:nrow(tax)){
  if   (str_detect(tax[row,7], paste(pattern, collapse="|")) && !is.na(exact$binomial[row])) {
  tax[row,7] <- exact$binomial[row]
  }
}

# Write taxonomy table to disk
write_rds(tax, "output/rds/tax_IdTaxaExact.rds") 

```

## Make Phyloseq object

Following taxonomic assignment, the sequence table and taxonomic table are merged into a single phyloseq object alongside the sample info csv.

We then make a plot to evaluate the effectiveness of taxonomic assignment to each rank

```{r create PS, eval = FALSE}
seqtab.nochim <- read_rds("output/rds/seqtab_final.rds")

##Fix seqtab names -removing read name, sample number etc
#rownames(seqtab.nochim) <- rownames(seqtab.nochim) %>% 
#  str_split_fixed("_",n=Inf) %>%
#    as_tibble() %>% 
#  separate(V7, into="rep", sep = "\\.", extra = "drop") %>%
#  unite(col=SampleID, c("V2","rep"),sep="-") %>%
#  pull(SampleID) %>%
#  str_replace(pattern="Rep", replacement="rep")
#

#Run 1 
rownames(seqtab.nochim) <- rownames(seqtab.nochim) %>% 
  str_split_fixed("_",n=Inf) %>%
    as_tibble() %>% 
  pull(V1)


### Rename problematic samples
rownames(seqtab.nochim)  <- rownames(seqtab.nochim) %>%
  str_replace_all("CM9", "CM8")%>%
  str_replace_all("CM10", "CM9")%>%
  str_replace_all("CM11", "CM10")%>%
  str_replace_all("CML1", "CM11")%>%
  str_replace_all("CML2", "CML1")%>%
  str_replace_all("CML3", "CML2")%>%
  str_replace_all("CML4", "CML3")%>%
  str_replace_all("CML5", "CML4")%>%
  str_replace_all("CML6", "CML5")%>%
  str_replace_all("CT1", "CML6")%>%
  str_replace_all("CT2", "CT1")%>%
  #str_replace_all("CT3", "CT2")%>%
  #str_replace_all("CT4", "CT3")%>%
  str_replace_all("CT5", "CT11")%>% # NEED TO CHECK THIS
  str_replace_all("CT5dup", "CT5")


tax_plus <- read_rds("output/rds/tax_IdTaxaExact.rds") 

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE) %>%
  filter(!duplicated(sample_id)) %>%
  filter(FCID== "HLVKYDMXX") %>% # change for other runs
  magrittr::set_rownames(.$sample_id) %>%
  dplyr::select(c("sample_id", "ExtractID",
                  "geo_loc_name", "material", "treatment", "material",
                  "target_subfragment", "F_primer", "R_primer",
                  "FCID", "seq_platform_ID"))
#Display samDF
head(samdf)

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax_plus), sample_data(samdf),
               otu_table(seqtab.nochim, taxa_are_rows = FALSE))

if(nrow(seqtab.nochim) > nrow(sample_data(ps))){warning("Warning: All samples not included in phyloseq object, check sample names match the sample metadata")}

rownames(samdf)[which(!rownames(sample_data(ps))  %in% rownames(samdf))]

## Subset to carpophilus

#Rename synthetic orders
tax_table(ps)[,2][which(str_detect(tax_table(ps)[,7], "Synthetic"))] <- "Arthropoda"

ps <- ps %>%
  subset_samples(material %in% c("Carpophilus Adults","Carpophilus Larvae")) %>%
  subset_taxa(Phylum == "Arthropoda") %>%
  filter_taxa( function(x) mean(x) > 0, TRUE) 

#rm_samples <- "CT11|CM7"
#ps <- subset_samples(ps, str_detect(sample_names(ps), !rm_samples)) # Drop Undetermined reads

##save phyloseq object
write_rds(ps, "output/rds/ps_carpophilus.rds") 

dir.create("output/csv")
dir.create("output/csv/unfiltered/")

##Export raw csv
export <- speedyseq::psmelt(ps) %>%
  filter(Abundance > 0)
write.csv(export, file = "output/csv/rawdata.csv")

#Summary export
library(data.table)
sp_summary <- summarise_taxa(ps, "Species", "sample_id")
sp_summary <- spread(sp_summary, key="sample_id", value="totalRA")
write.csv(sp_summary, file = "output/csv/unfiltered/spp_sum.csv")

gen_summary <- summarise_taxa(ps, "Genus", "sample_id")
gen_summary <- spread(gen_summary, key="sample_id", value="totalRA")
write.csv(gen_summary, file = "output/csv/unfiltered/gen_sum.csv")

##Output fasta of all ASV's - Name each one by abundance + Taxonomic assignment

ps_to_fasta(ps, "output/all_taxa.fasta", rank="Species")
```


### Summarise taxonomic assignment

```{r sum taxa}
#Fraction of reads assigned to each taxonomic rank
sum_reads <- speedyseq::psmelt(ps) %>%
  gather("Rank","Name",rank_names(ps)) %>%
  group_by(Rank) %>% 
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(Reads_classified = sum(Abundance * !is.na(Name))) %>%
  mutate(Frac_reads = Reads_classified / sum(sample_sums(ps))) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)

#Fraction of ASV's assigned to each taxonomic rank
sum_otu <- tax_table(ps) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
  gather("Rank","Name",rank_names(ps)) %>%
  group_by(Rank) %>%
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(OTUs_classified = sum(!is.na(Name))) %>%
  mutate(Frac_OTUs = OTUs_classified / ntaxa(ps)) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)

print(sum_reads)
print(sum_otu)
```

This indicates a 


## Process replicates 

In this section we will estimate within sample consistency using Kulczynski distance, which is a presence/absense distance measure.

To ensure the reproducibility of detection, all PCR replicates that had a high Kulczynski distance to other replicates within the same sample were removed.

Following this, we only retained ASV's that were present in at least 2 different replicates from each sample

Adapted from Mike mclarens code: https://github.com/benjjneb/dada2/issues/745

This could probably go before taxonomic assignment?

```{r replicates}

#Handle replicataes
rm_samples <- c("Undetermined")
ps1 <- subset_samples(ps, sample_names(ps) !=rm_samples) # Drop Undetermined reads
ps1 <- prune_samples(sample_sums(ps1)>=20, ps1) # Drop empty samples

#Calculate for each primer
#ps1 <- subset_samples(ps1,target_subfragment=="fwhF2-fwhR2n")
#ps1 <- subset_samples(ps1,target_subfragment=="fwhF2-HexCOIR4")

#Calculate kulczynski distance - A presence absense measure of detection 

  kdi <- phyloseq::distance(ps1, method="kulczynski")
  
  kdimap <- as.data.frame(as.matrix(kdi)) %>%
    rownames_to_column() %>%
    gather(key="colname",value="Distance",-rowname)
  
  #Make heatmap plot
  gg.kdimap <- ggplot(data = kdimap, aes(x=rowname, y=colname, fill=-Distance)) + 
    geom_tile() + scale_fill_viridis() + 
    ggtitle(paste0("kulczynski distance")) + 
    theme(axis.text.x=element_text(angle=90,hjust=1),
          axis.title = element_blank(),
          legend.position = "none")

# Merge replicates
  ps.merged <- ps1 %>%
    merge_samples(group = "ExtractID")

## keeping only those ASVs that occur in 2 replicates
## Create a matrix of 0s and 1s indicating whether the taxon count should be
## allowed, or should be set to 0.
#ps.merged.ok <- ps1 %>%
#    transform_sample_counts(function (x) (x > 0) * 1) %>%  #Summarise presence/absense across reps
#    merge_samples(group = "ExtractID") %>% #Merge reps
#    transform_sample_counts(function (x) (x > 1) * 1) #Only keep those occuring in 2 or more replicates
#
###Test export
###    write.csv(psmelt(ps.merged.ok),"test.csv")
##    
### Multiply the counts by the 0-1 matrix
#newotu <- otu_table(ps.merged) * otu_table(ps.merged.ok)
#
#otu_table(ps.merged) <- otu_table(newotu, taxa_are_rows = FALSE)
#

#This loses the sample metadata - Need to add it agian
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE)  %>%
  filter(!duplicated(ExtractID))  %>%
  magrittr::set_rownames(.$ExtractID) %>%
  dplyr::select(c("sample_id", "ExtractID",
                  "geo_loc_name", "material", "treatment",
                  "target_subfragment", "F_primer", "R_primer",
                  "FCID", "seq_platform_ID"))

sample_data(ps.merged) <- samdf
ps.merged <- filter_taxa(ps.merged, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table


# After merging kulczynski distance 

  kdi2 <- phyloseq::distance(ps.merged, method="kulczynski")
  
  kdimap2 <- as.data.frame(as.matrix(kdi2)) %>%
    rownames_to_column() %>%
    gather(key="colname",value="Distance",-rowname)
  
  #Make heatmap plot
  gg.kdimap2 <- ggplot(data = kdimap2, aes(x=rowname, y=colname, fill=-Distance)) + 
    geom_tile() + scale_fill_viridis() + 
    ggtitle(paste0("kulczynski distance post replicate merge")) + 
    theme(axis.text.x=element_text(angle=90,hjust=1)) + 
    theme(axis.text.x=element_text(angle=90,hjust=1),
          axis.title = element_blank(),
          legend.position = "none")


#Summary export
library(data.table)
sp_summary <- summarise_taxa(ps.merged, "Species", "sample_id")
sp_summary <- spread(sp_summary, key="sample_id", value="totalRA")
write.csv(sp_summary, file = "output/csv/unfiltered/spp_sum.csv")

gen_summary <- summarise_taxa(ps.merged, "Genus", "sample_id")
gen_summary <- spread(gen_summary, key="sample_id", value="totalRA")
write.csv(gen_summary, file = "output/csv/unfiltered/gen_sum.csv")
  
```



## ASV Filtering

* remove samples with low reads
* Remove all OTUS with an abundance less than 0.01 (or index switch estimate)
  -This could be better estimated with an ROC curve of false positives and false negatives in mock communities?
* Remove OTUs that are found in less than 20% of samples- For Quantitative analysis not for detection


# Bias

As part of the mock community analysis, we wish to determine taxonomic bias by looking at observed vs expected reads. To do this, we load dummy sequence, taxonomy, and sample data tables and create a seperate phyloseq object, which will later be merged

This loads a dummy sequence table, taxonomy table, and sample data table and merges it into the existing phyloseq object

Conducted as per: https://mikemc.github.io/metacal/articles/tutorial.html


The clr-transformed values are scale-invariant; that is the same ratio is expected to be obtained in a sample with few read counts or an identical sample with many read counts, only the precision of the clr estimate is affected. 

The G(x) cannot be determined for sparse data without deleting, replacing or estimating the 0 count values. Fortunately, there are acceptable methods of dealing with 0 count values as both point estimates using zCompositionsR package

```{r Carpophilus bias}
#Remove anything not assigned below the root rank
ps.filt <- ps.merged %>%
  subset_samples(!str_detect(sample_names(ps.merged), "-s-"))

taxa_names(ps.filt) <- paste0("SV", seq(ntaxa(ps.filt)),"-",tax_table(ps.filt)[,7])

#Agglomerate to species and subset to primers
ps_bias <- ps.filt
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Carpophilus_dimidiatus/nr.dimidiatus")] <- "Carpophilus_nr.dimidiatus"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp1")] <- "Brachypeplus_Sp"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp2")] <- "Brachypeplus_Sp"
ps_bias <- tax_glom(ps_bias, taxrank="Species")


sam <- psmelt(ps_bias) %>%
  arrange(Abundance)%>%
  mutate(Taxon = Species) %>%
  mutate(OTU = str_replace_all(OTU, pattern="Brachypeplus_Sp1", replacement = "Brachypeplus_Sp")) %>%
  mutate(OTU = str_replace_all(OTU, pattern="Brachypeplus_Sp2", replacement = "Brachypeplus_Sp"))

exp <- read_csv("sample_data/expected_quant_EDIT.csv") %>%
  gather(Species, Abundance, -X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  filter(str_detect(X1,"CM|CT")) %>%
  drop_na() %>%
  set_colnames(c("Sample","Taxon","Actual"))

#Subset to only those taxa desired for estimation
controls <- c("CM1-ex1","CM2-ex1","CM3-ex1","CM5-ex1", "CM6-ex1", "CM6-ex2","CM9-ex1","CM9-ex2")
#controls <- c("CM6-ex1","CM6-ex2","CM7-ex1","CM7-ex2","CM9-ex1","CM9-ex2")
#Join tables 
joint <- sam %>%
  #filter(material =="Carpophilus Adults") %>%
  filter(Taxon %in% exp$Taxon) %>%
  mutate(Type = ifelse(Sample %in% controls, "Est", "Eval")) %>%
  left_join(exp, by = c("Sample","Taxon")) %>%
  mutate(Actual = replace_na(Actual, 0)) %>%
  mutate(Observed0 = (Abundance + 0.5) * (Actual > 0)) %>%
  mutate(Error = Observed0 / Actual) %>%
  distinct()

#Build error matrix

error_mat <- joint %>%
    #filter(Type=="Est") %>%
    #filter(material == "Carpophilus Adults") %>%
    #filter(Taxon %in% (joint %>% filter(Type =="Est") %>% filter(Actual > 0) %>% pull(Taxon) %>% unique())) %>%
  group_by(target_subfragment) %>%
    build_matrix(Sample, Taxon, Error) %>% 
  as.numeric()

### CALCULATE SE
#
N <- 1:20
names(N) <- N

reps <- map_dfr(N, ~bootrep_center(error_mat, R = 4e3, dist = "multinomial",
        N = ., method = "proj"),
    .id = "N") %>%
    dplyr::rename(Bhat = Center)

reps.summary <- reps %>%
    group_by(N, Taxon) %>%
    summarize(gm_mean = gm_mean(Bhat), gm_se = gm_sd(Bhat))

r <- range(reps.summary$gm_se)
names(r) <- r  %>% round(2)

gg.se <- ggplot(reps.summary, aes(x= as.numeric(N), y=gm_se, color = Taxon)) +
    geom_line(aes(group = Taxon), size=1) +
    geom_point() +
    scale_y_continuous(limits = range(c(1, r)), ) +
    scale_color_brewer(palette="Spectral") +
    #scale_color_manual(values = values.color, labels = tax_labeller) +
    #geom_rangeframe(color = "black") +
    #theme_bw() +
    labs(x = "Number of controls", y = "Geometric standard error", 
        title = "Standard error vs. number of control samples", 
        color = "Taxon")
gg.se


#Estimate bias - can use a denominator by sending a denominator to a vector
bias <- center(error_mat, enframe = TRUE, method="proj") %>%
    dplyr::rename(Bhat = Center)

#Estimate uncertainty in bias estimate
bootreps <- bootrep_center(error_mat,  method="proj") %>%
    dplyr::rename(Bhat = Center)

#Plot bootstraps
library(ggridges)
gg.boot <- ggplot(bootreps, aes(x = Bhat-1, y = Taxon, fill=Taxon)) +
    geom_density_ridges(scale = 4) + theme_ridges() +
    geom_vline(xintercept = 0, color = "black") +
  scale_y_discrete(expand = c(0.01, 0)) +   # will generally have to set the `expand` option
  scale_x_continuous(expand = c(0, 0)) + 
  ggtitle("Bootstrap error estimates") +
  scale_fill_brewer(palette = "Spectral")

#summarise bootstraps
bootreps.summary <- bootreps %>%
    group_by(Taxon) %>%
    summarize(Gm_mean = gm_mean(Bhat), Gm_se = gm_sd(Bhat)) 
bias0 <- left_join(bias, bootreps.summary, by = "Taxon") 


#Bar chart of bias for each taxa 
gg.barbias <- ggplot(bias0, aes(Taxon, y=Gm_mean-1,fill=Taxon)) +
    geom_bar(stat="identity")+
    geom_errorbar(aes(ymin = (Gm_mean-1) - (Gm_se-1), ymax = (Gm_mean-1) + (Gm_se-1), width=0.2)) +
    geom_point(aes(y=Gm_mean-1)) +
    scale_fill_brewer(palette="Spectral")+
    scale_colour_brewer(palette="Spectral") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0)) + 
  ylab("Bias") +
  #ggtitle(primers[i]) +
  expand_limits(y = c(-2, 4)) +
  theme(legend.position = "none")

bootreps.pw <- bootreps %>%
    compute_ratios(group_vars = ".id")
summary.pw <- bootreps.pw %>%
    group_by(Taxon.x, Taxon.y) %>%
    summarize(Gm_mean = gm_mean(Bhat), Gm_se = gm_sd(Bhat))

ratios <- joint %>%
  #filter(Type=="Est") %>%
  filter(material == "Carpophilus Adults") %>%
    compute_ratios %>%
    mutate(Pair = paste(Taxon.x, Taxon.y, sep = ":")) %>%
    filter(!is.nan(Error), Taxon.x < Taxon.y)

ratios.pred <- summary.pw %>%
    mutate(Pair = paste(Taxon.x, Taxon.y, sep = ":")) %>%
    filter(Taxon.x < Taxon.y)

ggplot(ratios, aes(Pair, Error, color = Sample)) +
    geom_hline(yintercept = 1, color = "grey") +
  geom_point(data = ratios.pred,
        aes(Pair, Gm_mean), inherit.aes = FALSE,
        shape = 3, size = 5, color = "black") +
    geom_pointrange(data = ratios.pred, aes(y = Gm_mean, 
            ymin = Gm_mean / Gm_se^2, ymax = Gm_mean * Gm_se^2), 
        color = "black") +
      geom_text(aes(label=Sample),position = position_jitter(width=0.25, height=0)) +
    scale_y_log10() +
    coord_flip()

```

# Calibration

## Mclaren model
```{R}
#Calibration
cal <- joint %>%
    left_join(bias, by = c("Taxon")) %>%
    dplyr::mutate(Calibrated = Abundance / Bhat) 

#To scale with spike in, multiply everything by difference between observed reads and expected COI copies for the spikes

# Could assume that all other taxa have a bias of 1 and just set them to that?
cal.prop <- cal %>%
    filter(Taxon %in% (joint %>% filter(Type =="Est") %>% filter(Actual > 0) %>% pull(Taxon) %>% unique())) %>%
    group_by(Sample) %>%
    mutate_at(vars(Abundance, Calibrated, Actual), ~ . / sum(.)) 

gg.clrcal <- cal.prop %>%
    gather("Type", "Proportion", "Abundance", "Calibrated", "Actual") %>%
    mutate(Type = factor(Type, c("Actual","Abundance", "Calibrated"))) %>%
  ggplot(aes(Type, Proportion, fill = Taxon)) +
    geom_col() +
    facet_wrap(material~Sample, drop=TRUE) +
    theme_bw() +
    scale_fill_brewer(palette = "Spectral") +
  ggtitle("Calibration using model of Mclaren et al 2019")

#also make another plot showing the bias in the estimates from the amount of controls used - see https://mikemc.github.io/mgs-bias-manuscript/analysis/costea2017-analysis.html

```

## Linear model
```{r Linear model}
## Linear model
main1 <- joint %>%
  filter(material == "Carpophilus Adults") %>%
  filter(Type=="Est") %>%
  #filter(Taxon %in% (joint %>% filter(Type =="Est") %>% filter(Actual > 0) %>% pull(Taxon) %>% unique())) %>%
  group_by(Sample) %>%
  mutate_at(vars(Abundance, Actual), ~ . / sum(.))%>%
  dplyr::select(Sample, Taxon, Abundance, Actual, material) %>% 
  ungroup()

bias.slm <- main1 %>%
    group_by(Taxon) %>%
    nest() %>%
    mutate(fit = map(data, ~lm(Abundance ~ 0 + Actual, data = .)),
           tidied = map(fit, broom::tidy)) %>%
  #unnest(data) %>%
  unnest(tidied) %>%
  select(Taxon, estimate, std.error)


gg.linearcal <- joint %>%
    left_join(bias.slm, by = c("Taxon")) %>%
    mutate(Calibrated = Abundance / estimate) %>%
    group_by(Sample) %>%
  filter(Taxon %in% (joint %>% filter(Type =="Est") %>% filter(Actual > 0) %>% pull(Taxon) %>% unique()))%>%
    mutate_at(vars(Abundance, Calibrated, Actual), ~ . / sum(.))  %>%
    gather("Type", "Proportion", "Abundance", "Calibrated", "Actual") %>%
    mutate(Type = factor(Type, c("Actual","Abundance", "Calibrated"))) %>%
  ggplot(aes(Type, Proportion, fill = Taxon)) +
    geom_col() +
    facet_wrap(~Sample, drop=TRUE) +
    theme_bw() +
    scale_fill_brewer(palette = "Spectral") +
  ggtitle("Calibration using linear model")

```


# Compare models
```{r Compare models}
#Plot expected vs observed

correction <- joint %>%
    left_join(bias0, by = c("Taxon")) %>%
    mutate(Calibrated = Abundance / Bhat) %>%
    left_join(bias.slm, by = c("Taxon")) %>%
    mutate(lm = Abundance / estimate) %>%
    group_by(Sample) %>%
  filter(Taxon %in% (joint %>% filter(Type =="Est") %>% filter(Actual > 0) %>% pull(Taxon) %>% unique()))%>%
    mutate_at(vars(Abundance, Calibrated, Actual, lm), ~ . / sum(.)) %>% 
    ungroup() %>%
    dplyr::select(Sample, Taxon, Actual, Calibrated, Abundance, lm) %>%
  rename(Observed = Abundance) %>%
  tidyr::gather(Type, Abundance, -Sample, -Taxon, -Actual)  %>%
  mutate(Type = factor(Type, levels = c("Observed", "lm", "Calibrated"))) %>%
  filter(!is.nan(Abundance), Actual > 0) %>%
  distinct()

#To scale with spike in, multiply everything by difference between observed reads and expected COI copies for the spikes

col <- c(Observed= "#f7f7f7", lm = "#ef8a62", Calibrated = "#67a9cf")

gg.cor <- ggplot(correction, aes(x=Actual,y=Abundance)) +
  geom_point(aes(fill=Type), size=3, alpha=0.8, shape=21, stroke=1, color="black") + 
  geom_abline(slope=1, intercept = 0) +
  stat_cor(aes(color=Type), label.x = 0.1)  + 
  facet_grid(~Taxon) +
  #xlim(0,1) + 
  #ylim(0,1) + 
  scale_fill_manual(values=col) + 
  scale_colour_manual(values=col) +
  #theme_bw() + 
  ylab("Observed")

#MSE
pred <- cal %>%
    mutate(`No bias` = 1) %>%
    rename(`Estimated bias` = Bhat) %>%
    gather("Bias_type", "Bhat", 
        `No bias`, `Estimated bias`) %>%
    mutate_by(c(Sample, Bias_type), Predicted = close_elts(Actual * Bhat)) %>%
    mutate(Bias_type = factor(Bias_type, 
            c("No bias", "Estimated bias")))


error <- pred %>%
    group_by(material, Bias_type) %>%
    filter(Actual > 0) %>%
    summarize(
        MSE.prop = mean((Observed0 - Predicted)^2),
        MSE.logit = mean((logit(Observed0) - logit(Predicted))^2),
        RMSE.logit = sqrt(mean( (logit(Observed0) - logit(Predicted))^2 )),
        )
error

## Need to display this with standard error rather than 
```


## Omic plotr

```{r plot}



#New function
library(tidyverse)
ps_to_omicplotr <- function(physeq){
  #taxa_names(physeq) <- seq(ntaxa(physeq))
  
  names <- sample_names(physeq)
  # Start with fast melt
  ranks <- rank_names(physeq)
  mdt <- fast_melt(physeq) %>%
    tidyr::unite("taxonomy", !!ranks, sep=";") 

  order <- c("taxaID", names, "taxonomy")
    # Summarize
  summarydt <- mdt %>%
    dplyr::group_by(SampleID, taxaID, taxonomy) %>%
    dplyr::summarise(count=sum(count)) %>%
    tidyr::spread(key=SampleID, value=count, fill="0") %>%
    dplyr::select(!!order) %>%
    dplyr::rename(`#OTU ID` = taxaID)
  return(summarydt)
}



#output files
omic_otu <- ps_to_omicplotr(ps.filt)
write_tsv(omic_otu, path="omic_otu.txt")

#Output metadata
omic_data <- samdf %>%
  rownames_to_column("#SampleID") %>%
  dplyr::filter(ExtractID %in% colnames(omic_otu)) %>%
  dplyr::select(-sample_id, -ExtractID)
write_tsv(omic_data, path="omic_data.txt")

library(omicplotR)
#omicplotr.run()

```

Filtering options - Recreate function in omicplotR::omicplotr.filter() - Or can i do this in phyloseq
```{r}
min.count <- 100
min.prop <- 0.01
max.prop <- 1
min.sum <- 1
min.reads <- 1000
taxselect <- 
taxoncheck <- 
arrowcheck <- FALSE
scale.slider <- 0
removenames <- FALSE
var.filt <- 0.01
abund <- 0
dist <- 1
clust <- 3
#vals <- 
#metaval <- 
```

# PCA
PCA from omicplotR

Could show ordination results pre and post clusteirng
```{r}

library(zCompositions)
library(ALDEx2)
library(omicplotR)

#read file
file <- "omic_otu.txt"

data <- read.table(
  "omic_otu.txt",
  header = TRUE,
  sep = "\t",
  stringsAsFactors = FALSE,
  quote = "",
  row.names = 1,
  check.names = FALSE,
  comment.char = "",
  na.strings = "")

#filter data
x.filt <- omicplotr.filter(data, min.reads = min.reads, min.count = min.count, min.prop = min.prop, max.prop = max.prop, min.sum = min.sum)

#Drop taxonomy

# Impute zeroes
data.0 <- zCompositions::cmultRepl(t(subset(x.filt, select=-c(taxonomy))), label = 0, method = "CZM")

#or pseudocount zeroes
#data.0 <- t(x.filt + 0.5)

#data.0 <- t(subset(x.filt, select=-c(taxonomy)))
#Conduct CLR - Could try doing rowmeans of the spike ins instead?
x.clr <- as.matrix(log(data.0) - rowMeans(log(data.0)))

#Conduct variance filtering
var.clr <- matrixStats::colVars(x.clr)
names(var.clr) <- colnames(x.clr)
names.hvar <- names(var.clr)[which(var.clr > var.filt)]
x.clr.var <- x.clr[, names.hvar]

#PCA
pcx <- prcomp(x.clr.var)

#calculate variance for x/y axis of plot
x.var <- sum(pcx$sdev ^ 2)
PC1 <- paste("PC 1 Variance: %", round(sum(pcx$sdev[1] ^ 2) / x.var * 100, 1))
PC2 <- paste("PC 2 Variance: %", round(sum(pcx$sdev[2] ^ 2) / x.var*100, 1))

#get species (or other level)
spp <- x.filt$taxonomy %>%
  str_split_fixed(pattern=";", n=7) %>%
  as.data.frame() %>%
  pull(V7)

#colour of points (features black, samples red)
col = c("black", rgb(0, 0, 0, 0.2))

#add or change options for biplot.
biplot(
  pcx,
  main = "Principal Component Analysis",
  cex.main = 1.5,
  cex =  c(1.0, 0.8),
  col = col,
  scale = scale.slider,
  var.axes = TRUE,
  xlab = PC1,
  ylab = PC2,
  xlabs = unlist(dimnames(pcx$x)[1]),
  ylabs = spp
)

#Make scree plot
screeplot(pcx, type = "barplot", main="Scree plot")


```

# Figure 2 - RA dendogram and bars
Dendrogram

```{r RA Dendogram and bars}
library(ggdendro)
library(dendextend)


#filter data
x.filt <- omicplotr.filter(data, min.reads = min.reads, min.count = min.count, min.prop = min.prop, max.prop = max.prop, min.sum = min.sum)


d <- x.filt[, seq_along(x.filt) - 1]
taxon <- x.filt[(dim(x.filt)[2])]

# sum counts by name
d.agg <- aggregate(d, by=list(spp), FUN=sum)
tax.agg <- d.agg$Group.1
d.agg$Group.1 <- NULL

# convert to abundances
d.prop <- apply(d.agg, 2, function(x){x/sum(x)})

#filters by abundance (slider bar)
d.abund <- d.agg[apply(d.prop, 1, max) > abund,]
tax.abund.u <- tax.agg[apply(d.prop, 1, max) > abund]

d.abund <- t(cmultRepl(t(d.abund), label=0, method="CZM"))

# get proportions of the filtered data for plotting below
# in log-ratio speak, you are re-closing your dataset
d.P.u <- apply(d.abund, 2, function(x){x/sum(x)})

# order by OTU abundances
new.order <- rownames(d.P.u)[order(apply(d.P.u, 1, sum), decreasing=TRUE)]
tax.abund <- tax.abund.u[order(apply(d.P.u, 1, sum), decreasing=TRUE)]
d.P <- d.P.u[new.order, ]
d.clr <- apply(d.P, 2, function(x){log2(x) - mean(log2(x))})

#distance matrix
dist.d.clr <- dist(t(d.clr), method="euclidean")

#Cluster
clust.d <- hclust(dist.d.clr, method="ward.D2")

#plot the dendrogram 
dend <- as.dendrogram(clust.d)

# Rectangular lines
ddata <- dendro_data(dend, type = "rectangle")
gg.dend <- ggplot(segment(ddata)) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend))  + 
  scale_y_reverse(expand = c(0, 0)) +
  theme_void()+
  scale_x_discrete(expand=c(0.001,0.001))+
  coord_flip()

ps.bar <- ps.filt %>%
  speedyseq::tax_glom(taxrank = "Species") %>%           # agglomerate at species level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance
  speedyseq::psmelt() %>% 
  mutate(Species = as.character(Species)) %>%
  mutate(labels = Species) %>%
  filter(Abundance > 0.01) %>%
  #mutate(labels = case_when(
  #  Abundance >= 0.01 ~ Species, # Change this to whatever taxrank we want
  #  Abundance < 0.01 ~ "Other"
  #  )) %>%
  mutate(ExtractID = factor(ExtractID, levels=labels(dend)))


library(RColorBrewer)
col <- colorRampPalette(brewer.pal(11, "Spectral"))(25)

gg.bar <- ggplot(ps.bar, aes(x = ExtractID, y = Abundance, fill = labels)) + 
  geom_bar(stat = "identity", width = 0.8) +
  theme_minimal()+
  theme(axis.title = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "right",
        plot.margin=unit(c(-2,-1,10,0),unit="cm")) +
  scale_x_discrete(expand=c(0,0), position="top")+
  scale_y_discrete(expand=c(0,0))+
  scale_fill_manual(values=col) +
  guides(fill=guide_legend(ncol=1)) +
  coord_flip() 


#plot together

Fig2 <- gg.dend + gg.bar + plot_layout(ncol = 2, widths = c(1,3)) + plot_annotation(title="Beta diversity", tag_levels="A")


```


# Figure 3 - Differential abundance

```{r}
#these conditions are for the selex dataset
colnames(x.filt)


#filter data
x.filt <- omicplotr.filter(data, min.reads = min.reads, min.count = min.count, min.prop = min.prop, max.prop = max.prop, min.sum = min.sum)


library(tidyverse)
x.filt <- x.filt %>% dplyr::select(-taxonomy)

conds <- vector("numeric", length=length(x.filt))


conds[which(str_detect(colnames(x.filt), "CT"))] <- 0
conds[which(str_detect(colnames(x.filt), "CM"))] <- 1



#use ALDEx2 to for differential abundance analysis
#change MC or denominator if desired. More (1000-2000) MC samples is
# recommended, but takes longer to calculate.
d.clr <- aldex.clr(x.filt, mc.samples=128, conds = conds, denom = "all", verbose=TRUE)

#effect plot
aldex.plot(d.clr, type="MW", test="welch", all.cex = 1.5, rare.cex = 1.5, called.cex = 1.5, xlab = "Dispersion", ylab = "Difference")

title(main = "Effect Plot")

#Bland-Altman plot
aldex.plot(d.clr, type="MA", test="welch", all.cex = 1.5, rare.cex = 1.5, called.cex = 1.5, xlab = "CLR abundance", ylab = "Difference")
title(main = "Bland-Altman Plot")

######### save both in one pdf


count <- 1
aldex.in <- data.frame(d.r[which(apply(d.r, 1, function(x){mean(x)}) > count),], 
    check.names=F)

# Make a list of the number of ak samples and the number of op samples
# NOTE: This only works if all your samples are ordered with one group 
# first and the other group following
# If your table is not set up this way, you will have to order the columns

ak<-rep("ak", length(grep("ak", colnames(aldex.in))))
op<-rep("op", length(grep("op", colnames(aldex.in))))

#Combine into one vector
conds<-c(ak,op)

## CHECK this vector in your console by typing: conds
#    This should match the order and number of columns you have: colnames(aldex.in)

# Shortform way to do this:
#conds<-c(rep("ak", length(grep("ak", colnames(aldex.in)))), rep("op", length(grep("op", colnames(aldex.in)))))

# This is the main ALDEx function for all downstream analyses
# This is the number of Monte-Carlo samples from the Dirichlet distribution
# mc.samples=128 is often sufficient. 

x <- aldex.clr(aldex.in, conds, mc.samples=128, verbose=TRUE)

#Perform t-test
# Both Welches and Wilcoxon rank-sum t-test, plus a Benjamini-Hochberg multiple test correction
x.tt <- aldex.ttest(x, conds, paired.test=FALSE)

# Calculate effect size (standardized mean difference)
x.effect <- aldex.effect(x, conds, include.sample.summary=FALSE, verbose=TRUE)

#COmbine results
x.all <- data.frame(x.tt, x.effect, stringsAsFactors=FALSE)

#Effect plot
aldex.plot(x.all, type="MA", test="welch")

aldex.plot(x.all, type="MW", test="welch")


#Significant OTU's

# Get the OTUs with a Benjamini-Hochberg corrected p-value less than 0.05, 
# AND an effect size greater than 1
# OTUs with a higher relative abundance in op will have positive effect 
# values, OTUs higher in ak will have negative effect values
# we can use Wilcoxon test results: wi.eBH, or raw p values (wi.ep, we.ep)
# from either test
sig.ak <- rownames(x.all)[which(x.all$we.eBH < 0.05 & x.all$effect < -1)]
sig.op <- rownames(x.all)[which(x.all$we.eBH < 0.05 & x.all$effect > 1)]

# What are the taxa that are significantly different?
# Using the OTU names, we can get the taxonomy from our taxon table
taxon[sig.ak,]

# We can also look at the ALDEx output for the significant OTUs
x.all[sig.ak,]

```


## Process synthetic mock positive control

Somehow losing spikeins here
```{r}
#Process positive controls
Syn_taxa <- c("Synthetic_Acrididae", "Synthetic_Aphididae", "Synthetic_Apidae", "Synthetic_Cerambycidae", "Synthetic_Crambidae", "Synthetic_Culicidae","Synthetic_Drosophilidae","Synthetic_Nitidulidae","Synthetic_Siricidae","Synthetic_Tephritidae", "Synthetic_Thripidae", "Synthetic_Tortricidae", "Synthetic_Triozidae")
#Estimate switching from positive controls


syn_reads <- speedyseq::psmelt(ps.merged) %>%
  filter(Species %in% Syn_taxa) %>%
  group_by(geo_loc_name) %>%
  summarise(Abundance = sum(Abundance))

syn_switching <- (syn_reads %>%
                     filter(!str_detect(geo_loc_name, "Synthetic")) %>% 
                     pull(Abundance) %>%
                     sum()) / 
                  (syn_reads %>%
                      filter(str_detect(geo_loc_name, "Synthetic")) %>%
                    pull(Abundance)) 

##Plot synthetics
ps_syn <- subset_samples(ps.merged, geo_loc_name=="Synthetic")
ps_syn <- subset_taxa(ps_syn,  Species %in% Syn_taxa)

ps_syn <- filter_taxa(ps_syn, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
ps_syn_prop <- transform_sample_counts(ps_syn, fun = proportions,na_rm=TRUE) # Breaking agglomeration

#Colour scheme
library(RColorBrewer)
col <- colorRampPalette(brewer.pal(11, "Spectral"))(13)

gg.syn <- plot_bar(ps_syn, fill="Species") +
  facet_grid(~target_subfragment,scales="free") + 
  scale_fill_manual(values=col) +
  theme_bw() +
  theme(axis.text.x=element_text(angle=90),
        legend.position = "none")

gg.propsyn <- plot_bar(ps_syn_prop, fill="Species") +
  facet_grid(~target_subfragment,scales="free") + 
  scale_fill_manual(values=col)+
  theme_bw() +
  theme(axis.text.x=element_text(angle=90))

gg.syn + gg.propsyn
```

# Analyse spike ins for run 3
```{r analyse spike ins}
library(plotly)
library(patchwork)

#Remove blanks
rm_samples <- sample_names(ps.merged)[which(str_detect(sample_names(ps.merged),"BLANK"))]
ps1 <- subset_samples(ps.merged, !sample_names(ps.merged) %in% rm_samples) # Drop Undetermined reads
ps1 <- prune_samples(sample_sums(ps1)>=20, ps1) # Drop empty samples

#Replace experimental factor column with spikeins name to allow faceting by
spikeins <- c("DL1","DL2","DL3","DL4","DL5","DL6","CL1","CL2","CL3","CL4","CL5","CL6")

sample_data(ps1)$experimental_factor <- as.character(sample_data(ps1)$experimental_factor)

for (i in 1:length(spikeins)){
  sample_data(ps1)$experimental_factor[which(str_detect(sample_data(ps1)$ExtractID,spikeins[i]))] <- spikeins[i]
}

#Add dataset column
sample_data(ps1) <- data.frame(sample_data(ps1)) %>%
  mutate(dataset = str_replace(experimental_factor,pattern=".$",replacement=""))%>%
  set_rownames(.$ExtractID)

psprop <- transform_sample_counts(ps1, fun = proportions,na_rm=FALSE) # Breaking agglomeration


#only highlight syn & others
tax.other <- data.frame(tax_table(psprop))
for (i in 1:7){ tax.other[,i] <- as.character(tax.other[,i])}
tax.other$Kingdom[which(str_detect(tax.other$Species,"_Synthetic"))] <- "Synthetic"
tax.other$Kingdom[which(!str_detect(tax.other$Species,"_Synthetic"))] <- "Other"
tax_table(psprop) <- as.matrix(tax.other)

gg.bardros <- plot_bar(subset_samples(psprop,dataset=="DL"), fill="Kingdom") +facet_grid(dataset~experimental_factor,scales="free",drop=TRUE) +
  scale_fill_manual(values=c("#ef8a62","#67a9cf")) +
  theme_bw() +
  theme(axis.text.x=element_text(angle=90))

gg.barcarp <- plot_bar(subset_samples(psprop,dataset=="CL"), fill="Kingdom") +facet_grid(dataset~experimental_factor,scales="free",drop=TRUE) +
  scale_fill_manual(values=c("#ef8a62","#67a9cf")) +
  theme_bw() +
  theme(axis.text.x=element_text(angle=90))

gg.bardros / gg.barcarp

#add extra column to refer to this, str_detect the spike in name ie DL1, CL1 etch

#then plot that similar heatmap of detection of the spike ins
#could we do this with a summarise taxa call then subset to synthetics only?

test <- summarize_taxa(psprop, "Species", "ExtractID") %>%
  dplyr::filter(str_detect(Species,"_Synthetic")) %>%
  dplyr::filter(!str_detect(ExtractID,"BLANK")) %>%
  mutate(experimental_factor="O") 

spikeins <- c("DL1","DL2","DL3","DL4","DL5","DL6","CL1","CL2","CL3","CL4","CL5","CL6")

for (i in 1:length(spikeins)){
  test$experimental_factor[which(str_detect(test$ExtractID,spikeins[i]))] <- spikeins[i]
}

#add dataset column by removing last character of experimental factor
test <- test %>%
  mutate(dataset = str_replace(experimental_factor,pattern=".$",replacement=""))

gg.spikedros <- ggplot(test %>% filter(dataset=="DL"),aes(x=ExtractID,y=Species,fill=totalRA)) + 
  geom_tile()  +
  scale_fill_viridis() + 
  facet_grid(dataset~experimental_factor,scales="free",drop=TRUE) +
  theme_bw()+
  theme(axis.text.x = element_text(angle=90))

gg.spikecarp <- ggplot(test %>% filter(dataset=="CL"),aes(x=ExtractID,y=Species,fill=totalRA)) + 
  geom_tile() +
  theme(axis.text.x = element_text(angle=90)) +
  scale_fill_viridis() + 
  facet_grid(dataset~experimental_factor,scales="free",drop=TRUE) +
  theme_bw()+
  theme(axis.text.x = element_text(angle=90))

gg.spikedros / gg.spikecarp

```

