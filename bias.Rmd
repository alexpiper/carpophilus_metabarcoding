---
title: "Metabarcoding bias"
output: html_document
editor_options: 
  chunk_output_type: console
---
# Introduction

## Analysis structure

Part 1 - Cleanup & Filtering

Part 1 - Comparison of 4 primers for bias

Part 2 - Evaluation of different bias modellign strategies

- naive mean on proportions approach
- Metacal center function
  - proj
  - gm
  - rss
- lm on compositional vectors
- lm on expected / observed
- lm with CLR in advance?
- glmnet: multinomial regression with regularization (ie Lasso model)
- nnet::multinomial neural network regression
- multinomial regression with random effects
- Poisson regression with random effects
- Bayesian multinomial logistic normal models
  - Stan
  - Pibble from stray package
  
Look at spike in control

Pick the model that reduces the residual mean standard error (RMSE) the most

Should be able to do this within a tidymodels framework with bootsrapping

## Load packages
```{r setup}
#Set required packages
.cran_packages <- c("tidyverse",
                    "tidymodels",
                    "patchwork", 
                    "vegan", 
                    "seqinr",
                    "ape", 
                    "RColorBrewer",
                    "devtools",
                    "data.table")
.bioc_packages <- c("dada2",
                    "phyloseq", 
                    "ALDEx2",
                    "Biostrings")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

# Github packages
#devtools::install_github("alexpiper/taxreturn")
#devtools::install_github("mikemc/speedyseq")
#devtools::install_github("mikemc/metacal")

library(taxreturn)
library(speedyseq)
library(metacal)

#Source themes
source("R/themes.R")
source("R/helper_functions.R")
```

## Make Phyloseq object

```{r create PS, eval = FALSE}
seqtab <- readRDS("output/rds/seqtab_final.rds")

#Reformat sample IDs
rownames(seqtab)  <- rownames(seqtab) %>%
  str_remove("\\..*$") %>%
  str_replace("\\_S[0-9].*\\_...", replacement="_") %>%
  str_replace("_$", "_1") %>%
  str_replace("1in10", "1:10")

tax <- readRDS("output/rds/final_tax.rds") 
seqs <- DNAStringSet(colnames(seqtab))
names(seqs) <- seqs
phy <- read.tree("output/ultrametric_tree_order_constrained.nwk")

##### Rename problematic samples
rownames(seqtab)  <- rownames(seqtab) %>%
 str_replace_all("CM10-", "CM9REP-") %>%
 str_replace_all("CM11-", "CM10REP-") %>%
 str_replace_all("CM9-", "CM11REP-") %>%
 str_replace_all("CML2-", "CML6REP-")%>%
 str_replace_all("CML3-", "CML2REP-")%>%
 str_replace_all("CML4-", "CML3REP-")%>%
 str_replace_all("CML5-", "CML4REP-")%>%
 str_replace_all("CML6-", "CML5REP-")%>%
 str_replace_all("CT5-", "CT4REP-")%>%
 str_replace_all("CT4-", "CT5REP-") %>%
str_replace_all("REP", "")


# Samdf processing --------------------------------------------------------
samdf <- read.csv("sample_data/sample_info.csv", header=TRUE) %>% 
  mutate(sample_id = case_when(
    fcid=="HLVKYDMXX" ~ paste0(sample_name, "_", amp_rep),
    !fcid=="HLVKYDMXX" ~ paste0(fcid, "_", sample_name, "_", amp_rep)
  )) %>%
  mutate(extract_id = sample_name) %>%
  mutate(sample_name = str_remove(sample_name, "-exp*.$")) %>%
  mutate(sample_name = case_when(
    fcid=="HLVKYDMXX" ~ sample_name,
    !fcid=="HLVKYDMXX" ~ paste0(fcid, "_", sample_name)
  )) %>%
  filter(!(i7_index=="ATCGATCG" & i5_index=="ATCACACG"), #CT11-ex1 duplicated
         !(i7_index=="TCGCTGTT" & i5_index=="ACTCCATC") # CT12-ex1 duplicated
  ) %>%
  mutate(type = case_when(
    str_detect(sample_id, "D[0-9][0-9][0-9]M|D[0-9][0-9][0-9][0-9]M|DM[0-9]")  ~ "DrosMock",
    str_detect(sample_id, "SPD")  ~ "SPD",
    str_detect(sample_id, "ACV")  ~ "ACV",
    str_detect(sample_id, "DC")  ~ "DC",
    str_detect(sample_id, "Sach")  ~ "Sachet",
    str_detect(sample_id, "FF")  ~ "FF",
    str_detect(sample_id, "NTC")  ~ "NTC",
    str_detect(sample_id, "DLarv")  ~ "DrosLarv",
    str_detect(sample_id, "POS|SynMock")  ~ "POS",
    str_detect(sample_id, "extblank|BLANK")  ~ "Extblank",
    str_detect(sample_id, "pcrblank")  ~ "PCRblank",
    str_detect(sample_id, "CT")  ~ "CarpTrap",
    str_detect(sample_id, "CM[0-9]")  ~ "CarpMock",
    str_detect(sample_id, "CML[0-9]")  ~ "CarpLarval"
  )) %>%
  magrittr::set_rownames(.$sample_id)

write_csv(samdf, "sample_data/sample_info2.csv")

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/sample_info2.csv", header=TRUE) %>%
  magrittr::set_rownames(.$sample_id)

# Will probably need to rename the seqtabs and append the flowcell number onto the samples before they are merged

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax), 
               sample_data(samdf),
               otu_table(seqtab, taxa_are_rows = FALSE),
               phy_tree(phy),
               refseq(seqs))

if(nrow(seqtab) > nrow(sample_data(ps))){warning("Warning: All samples not included in phyloseq object, check sample names match the sample metadata")}

rownames(samdf)[which(!rownames(sample_data(ps))  %in% rownames(samdf))]
rownames(sample_data(ps))[which(!rownames(samdf)  %in% rownames(sample_data(ps)))]

# Rename all taxa
taxa_names(ps) <- paste0("SV", seq(ntaxa(ps)),"-",tax_table(ps)[,8])

saveRDS(ps, "output/rds/ps_idtaxaExact.rds") 

#Rename synthetic spike ins
tax_table(ps)[,3][which(str_detect(tax_table(ps)[,8], "Synthetic"))] <- "Arthropoda"

# Rename incorrectly named taxa
tax_table(ps)[,8][which(tax_table(ps)[,8]=="Carpophilus dimidiatus/nr.dimidiatus")] <- "Carpophilus nr.dimidiatus"
tax_table(ps)[,8][which(tax_table(ps)[,8]=="Brachypeplus Sp1")] <- "Brachypeplus Sp"
tax_table(ps)[,8][which(tax_table(ps)[,8]=="Brachypeplus Sp2")] <- "Brachypeplus Sp"

#Subset full dataset to Carpophilus samples of interest
ps <- ps %>%
  subset_taxa(
    Phylum == "Arthropoda" &
    !Order == "Synthetic"
    )%>%
  subset_samples(str_detect(sample_name, "CM[0-9]|CT[0-9]|CML[0-9]")) %>%
  subset_samples(type %in% c("CarpMock",  "CarpTrap", "CarpLarval")) %>%
  subset_samples(pcr_primers == "fwhF2-fwhR2n") %>%
  subset_samples(fcid %in% c("HLVKYDMXX", "CB3DR")) %>%
  filter_taxa( function(x) mean(x) > 0, TRUE) %>%
  speedyseq::tax_glom(taxrank="Species") # Agglomerate to species

dir.create("output/csv")
dir.create("output/csv/unfiltered/")

# Export raw csv
export <- speedyseq::psmelt(ps) %>%
  filter(Abundance > 0)
write.csv(export, file = "output/csv/rawdata.csv")

# Summary export
seqateurs::summarise_taxa(ps, "Species", "sample_name") %>%
  filter(!str_detect(sample_name, "NTC")) %>%
  spread(key="sample_name", value="totalRA") %>%
  filter(!str_detect(Species, "__")) %>%
  write.csv(file = "output/csv/unfiltered/spp_sum.csv")

seqateurs::summarise_taxa(ps, "Genus", "sample_name") %>%
  spread(key="sample_name", value="totalRA") %>%
  filter(!str_detect(Genus, "__")) %>%
  write.csv(file = "output/csv/unfiltered/gen_sum.csv")

# Species detection summary
speedyseq::psmelt(ps) %>%
  filter(!str_detect(Genus, "__"), Abundance > 0 ) %>%
  mutate(abundance_ra = Abundance) %>%
  group_by(sample_id, fcid) %>%
  mutate_at(vars(abundance_ra), ~ . / sum(., na.rm = TRUE) ) %>%
  ungroup() %>%
  group_by(Species) %>%
  summarise(total_reads = sum(Abundance), mean_ra = mean(abundance_ra, na.rm=TRUE))%>%
  write.csv(file = "output/csv/unfiltered/detection_summary.csv")


# Output fasta of all ASV's - Name each one by abundance + taxonomic assignment
seqateurs::ps_to_fasta(ps, "output/all_taxa.fasta", seqnames = "Species")
```

## Minimum read filtering

```{r minimum reads}
#Plot rarefaction curve
rare <- otu_table(ps) %>%
  as("matrix") %>%
  rarecurve(step=10000) %>%
  purrr::map_dfr(., function(x){
  b <- as.data.frame(x)
  b <- data.frame(OTU = b[,1], count = rownames(b))
  b$count <- as.numeric(gsub("N", "",  b$count))
  nm <- names(attr(x, "Subsample"))
  b$sample_id <- nm[!nm==""]
  return(b)
  })%>%
  left_join(sample_data(ps)%>%
    as("matrix") %>%
    as_tibble() %>%
      dplyr::select(sample_id, fcid) %>%
      distinct())%>%
  dplyr::filter(fcid %in% c("CB3DR", "HLVKYDMXX"))

# threshold for read removal
threshold = 1000

gg.rare <- rare %>%
  ggplot() +
  geom_line(aes(x = count, y = OTU, group=sample_id), alpha=0.5)+
  geom_point(data = rare %>% 
               group_by(sample_id) %>% 
               top_n(1, count),
             aes(x = count, y = OTU, colour=(count > threshold))) +
  scale_x_log10(labels =  scales::label_number_si()) +
  geom_vline(xintercept=threshold, linetype="dashed") +
  facet_wrap(fcid~., scales="free", ncol=1)+
  base_theme+
  theme(legend.position = "bottom")+
  labs(x = "Sequence reads",
       y = "Observed ASV's",
       colour = "Sample retained?")
gg.rare

#Write out figure
pdf(file="fig/supplementary/rarefaction.pdf", width = 8, height = 6 , paper="a4r")
  plot(gg.rare)
try(dev.off(), silent=TRUE)

#Remove all samples under the minimum read threshold 
ps1 <- prune_samples(sample_sums(ps)>=threshold, ps) 
ps1 <- filter_taxa(ps1, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
message(nsamples(ps) - nsamples(ps1), " Samples and ", ntaxa(ps) - ntaxa(ps1), " taxa under read threshold Dropped")

# Check which were dropped
setdiff(sample_names(ps), sample_names(ps1))
```


### Summary statistics

```{r sum taxa}
exp_samples <- read_csv("sample_data/expected_quant_carpophilus.csv") %>%
  dplyr::rename(sample_name = X1) %>%
  pivot_longer(-sample_name,
               names_to= "taxon",
               values_to= "expected") %>%
  mutate(taxon = str_replace(taxon, pattern=" ",replacement="_"),
         sample_name = str_remove(sample_name, "-exp*.$")) %>%
  dplyr::filter(!is.na(sample_name),
  str_detect(sample_name, "CM[0-9]|CT[0-9]|CML[0-9]"),
  expected > 0) %>%
  distinct()

summary_dat <- speedyseq::psmelt(ps1) %>%
  filter(Abundance > 0 ) %>%
  dplyr::select(OTU, Sample, Abundance, sample_id, sample_name, extract_id, fcid, rank_names(ps1), type) %>%
  mutate(Genus = case_when(
    str_detect(Genus, "__") ~ as.character(NA),
    TRUE  ~ Species
  )) %>%
  mutate(Species = case_when(
    str_detect(Species, "__") ~ as.character(NA),
    TRUE ~ Species
  )) %>%
  mutate(abundance_ra = Abundance) %>%
  group_by(sample_id, fcid) %>%
  mutate_at(vars(abundance_ra), ~ . / sum(., na.rm = TRUE) ) %>%
  ungroup() %>% 
  mutate(above_thresh = case_when(
    abundance_ra < 1e-4 ~ FALSE,
    TRUE ~ TRUE
    )) %>%
  mutate(mock_spp = case_when(
    str_replace(Species, " ", "_") %in% exp_samples$taxon ~ TRUE,
    TRUE ~ FALSE
  ))

summary_dat%>%
  group_by(mock_spp) %>%
  summarise(n_extracts = n_distinct(extract_id), n_samples = n_distinct(sample_id),
            n_asv = n_distinct(OTU), n_spp = n_distinct(Species), n_genus = n_distinct(Genus))

summary_dat %>%
  filter(above_thresh, !str_detect(Species, "__")) %>%
  dplyr::select(Species, Order) %>%
  distinct() %>%
  pull(Order) %>%
  length()
  
  
# N unique species and samples
summary_dat %>%
  summarise(n_extracts = n_distinct(extract_id), n_samples = n_distinct(sample_name), n_reps = n_distinct(sample_id))


# Spread of reads
summary_dat %>%
  group_by(extract_id, fcid) %>%
  summarise(Abundance = sum(Abundance)) %>%
  ungroup() %>%
  group_by(fcid) %>%
  summarise(mean = mean(Abundance), 
            se = sd(Abundance)/sqrt(length(Abundance)),
            max = max(Abundance),
            min = min(Abundance),
            total = sum(Abundance))

# Spread of ASVs across samples - Add a grouping variable if they were in the mock communities, as well as if they were below 1e-4RA
ps1 %>%

  group_by(extract_id, fcid) %>%
  dplyr::filter(Abundance > 0) %>%
  summarise(counts = n_distinct(OTU)) %>%
  ungroup() %>%
  group_by(fcid) %>%
  summarise(mean = mean(counts), 
            se = sd(counts)/sqrt(length(counts)),
            max = max(counts),
            min = min(counts))

#Fraction of reads assigned to each taxonomic rank
speedyseq::psmelt(ps1) %>%
  gather("Rank","Name", rank_names(ps1)) %>%
  group_by(Rank) %>% 
  mutate(Name = replace(Name, str_detect(Name, "__"), NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  dplyr::summarise(Reads_classified = sum(Abundance * !is.na(Name))) %>%
  mutate(Frac_reads = Reads_classified / sum(sample_sums(ps1))) %>%
  mutate(Rank = factor(Rank, rank_names(ps1))) %>%
  arrange(Rank)

#Fraction of ASV's assigned to each taxonomic rank
tax_table(ps1) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
  gather("Rank","Name",rank_names(ps1)) %>%
  group_by(Rank) %>%
  mutate(Name = replace(Name, str_detect(Name, "__"), NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  dplyr::summarise(OTUs_classified = sum(!is.na(Name))) %>%
  mutate(Frac_OTUs = OTUs_classified / ntaxa(ps1)) %>%
  mutate(Rank = factor(Rank, rank_names(ps1))) %>%
  arrange(Rank)

# Unique taxa at each rank
speedyseq::psmelt(ps1) %>%
  dplyr::select(rank_names(ps1)) %>%
  pivot_longer(everything(),
               names_to = "Rank",
               values_to = "value") %>%
  mutate(value = case_when(
    str_detect(value, "__") ~ as.character(NA),
    !str_detect(value, "__") ~ value
  )) %>%
  drop_na() %>%
  group_by(Rank) %>%
  summarise_all(list(n_distinct)) %>%
  mutate(Rank = factor(Rank, rank_names(ps1))) %>%
  arrange(Rank)

```

# Taxon filtering

```{r}
get_taxa_unique(ps1, "Order")
ps1 # Check the number of taxa prior to removal

tax_to_keep <- exp_samples$taxon %>%
  unique() %>%
  str_replace("_", " ")

tax_to_keep[!tax_to_keep %in% as(tax_table(ps1),"data.frame")$Species]

ps2 <- ps1 %>%
  subset_taxa(Species %in% tax_to_keep)

# Summary after filtering 


# Spread of reads
ps2 %>%
  speedyseq::psmelt()%>%
  group_by(extract_id, fcid) %>%
  summarise(Abundance = sum(Abundance)) %>%
  ungroup() %>%
  group_by(fcid) %>%
  summarise(mean = mean(Abundance), 
            se = sd(Abundance)/sqrt(length(Abundance)),
            max = max(Abundance),
            min = min(Abundance),
            total = sum(Abundance))
```



## Success of replicates

```{r rep success }
exp_samples <- read_csv("data/HLVKYDMXX/SampleSheet.csv", skip = 19) %>%
  dplyr::select(extract_id = Sample_Name, sample_name = Sample_Name) %>%
  group_by(extract_id) %>%
  #group_split() %>%
  group_modify(~{
    .x %>%
      dplyr::slice(rep(1:n(), each=3)) %>%
      mutate(amp_rep = row_number()) %>%
      mutate(sample_id = paste0(sample_name, "_", amp_rep))
  }) %>%
  ungroup() %>%
  mutate(sample_name = extract_id %>%
           str_remove("-ex[0-9]$"))%>%
  mutate(type = case_when(
    str_detect(sample_id, "D[0-9][0-9][0-9]M|D[0-9][0-9][0-9][0-9]M|DM[0-9]")  ~ "DrosMock",
    str_detect(sample_id, "SPD")  ~ "SPD",
    str_detect(sample_id, "ACV")  ~ "ACV",
    str_detect(sample_id, "DC")  ~ "DC",
    str_detect(sample_id, "Sach")  ~ "Sachet",
    str_detect(sample_id, "FF")  ~ "FF",
    str_detect(sample_id, "NTC")  ~ "NTC",
    str_detect(sample_id, "DLarv")  ~ "DrosLarv",
    str_detect(sample_id, "POS|SynMock")  ~ "POS",
    str_detect(sample_id, "extblank|BLANK")  ~ "Extblank",
    str_detect(sample_id, "pcrblank")  ~ "PCRblank",
    str_detect(sample_id, "CT")  ~ "CarpTrap",
    str_detect(sample_id, "CM[0-9]")  ~ "CarpMock",
    str_detect(sample_id, "CML[0-9]")  ~ "CarpLarval"
  )) %>%
  filter(type %in% c("CarpMock", "CarpLarval", "CarpTrap"))

# Heatmap of abundance of pcr replicates
rep_data <- exp_samples %>%
  left_join(ps2 %>% 
  speedyseq::psmelt() %>%
    filter(Abundance > 0) %>%
    group_by(sample_name, extract_id, sample_id, amp_rep, fcid, type) %>%
    summarise(Abundance = sum(Abundance), n_spp = n_distinct(Species)) %>%
    ungroup()) %>%
  mutate(extrep = extract_id %>% str_extract("ex.*$")) %>%
  #filter(!(extrep=="ex1" & amp_rep == 2)) %>%
  mutate(amp_rep = factor(amp_rep)) %>%
  mutate(extrep = extrep %>% str_replace("ex", "Extraction Rep ")) %>%
  filter(amp_rep %in% 1:3)

# Summarise of replicate success
rep_data %>%
  ungroup() %>%
  mutate(success = case_when(
    Abundance > 0 ~ TRUE,
    Abundance == 0 | is.na(Abundance) ~ FALSE
  )) %>%
  group_by(type) %>%
  summarise(sum=sum(success), total = n()) %>%
  mutate(proportion = sum / total)

# Summary of sample success
rep_data %>%
  ungroup() %>%
  group_by(sample_name, type) %>%
  summarise(Abundance = sum(Abundance, na.rm = TRUE))%>%
  ungroup() %>%
  mutate(success = case_when(
    Abundance > 0 ~ TRUE,
    Abundance == 0 | is.na(Abundance) ~ FALSE
  )) %>%
  group_by(type) %>%
  summarise(sum=sum(success), total = n()) %>%
  mutate(proportion = sum / total)


gg.repmap <- rep_data %>%
   ggplot(aes(x = amp_rep, y = sample_name, fill = n_spp)) +
  geom_tile()+
  scale_fill_viridis_c()+
  facet_grid(extrep~type, scales="free", space="free", drop=TRUE)+
  base_theme+
  scale_x_discrete(expand=c(0,0))+
  scale_y_discrete(expand=c(0,0))+
  coord_flip()+
  theme(legend.position = "right")+
  labs(x = "PCR Replicate",
       y = NULL,
       fill = "N Species")
gg.repmap
```

# Fig2 - Error consistency

Look at bias consistency between replicates

# Visualise errors between samples & reps

```{r Consistency between reps}
exp <- read_csv("sample_data/expected_quant_carpophilus.csv") %>%
  dplyr::rename(sample_name = X1) %>%
  pivot_longer(-sample_name,
               names_to= "taxon",
               values_to= "expected") %>%
  mutate(taxon = str_replace(taxon, pattern=" ",replacement="_"),
         sample_name = str_remove(sample_name, "-exp*.$")) %>%
  dplyr::filter(!is.na(sample_name),
  str_detect(sample_name, "CM[0-9]|CT[0-9]|CML[0-9]"),
  expected > 0) %>%
  distinct()

#Get obsered
sam <- speedyseq::psmelt(ps2) %>%
  janitor::clean_names() %>%
  filter(!str_detect(species, "__")) %>%#Remove unclassified
  mutate(taxon = species %>% str_replace(" ", "_")) %>%
  dplyr::select(sample_name, extract_id, taxon, abundance, amp_rep, pcr_primers, fcid, material_type = type) %>%
  mutate(sample_name = sample_name %>%
           str_remove_all("BF1-BR1-|SauronS878-HexCOIR4-|fwhF2-HexCOIR4-|fwhF2-fwhR2n-") %>%
           str_remove("HLVKYDMXX_") %>%
           str_remove("CB3DR_"),
         sample_id = paste0(fcid, "_", sample_name),
         sample_name = sample_name %>%
           str_remove("-ex[0-9]")) %>%
  mutate(extrep = extract_id %>% str_extract("ex.*$")) %>%
  #filter(!(extrep=="ex1" & amp_rep == 2)) %>%
  mutate(amp_rep = factor(amp_rep)) %>%
  mutate(extrep = extrep %>% str_replace("ex", "Extraction Rep ")) %>%
  filter(amp_rep %in% 1:3)

#Join tables 
joint <- sam %>%
  filter(taxon %in% exp$taxon,
         sample_name %in% exp$sample_name) %>%
  left_join(exp, by = c("sample_name","taxon")) %>%
  dplyr::rename(observed = abundance) %>%
  filter(!is.na(expected))%>%
  mutate(expected = replace_na(expected, 0),
         observed0 = (observed + 0.5) * (expected > 0),
         error = observed0 / expected,
         #error_centre = center_elts(observed0 / expected, na.rm = TRUE),
         ) %>%
  distinct() %>%
  group_by(sample_id) %>%
  mutate(observed.prop=observed, expected.prop=expected) %>%
  mutate_at(vars(observed.prop,  expected.prop), ~ . / sum(.) ) %>% #Convert to proportions
  mutate(sample_size = sum(expected), # Do the same for the ratios?
         observed_abs = observed.prop * sample_size,
         error_abs = observed_abs / expected,
         error_prop = observed.prop / expected.prop
         )%>%
  filter(!sample_id=="HLVKYDMXX_CML6") 

# Original errors
error_rmse <- joint %>%
  group_by(extrep, amp_rep) %>%
  rmse(truth = expected.prop, estimate = observed.prop) %>%
  #rmse(truth = logit(expected.prop), estimate = logit(observed.prop)) %>%
  dplyr::select(RMSE = .estimate, extrep, amp_rep)

error_anorm <- joint %>%
  group_by(material_type) %>%
    summarise(Adist = anorm(observed.prop / expected)) 

## Visualise errors in proportions
gg.error_props <- joint %>% 
  filter(expected > 0) %>%
  #mutate_at(vars(expected.prop, observed.prop), logit) %>%
  left_join(error_rmse) %>%
  left_join(error_anorm) %>%
  mutate(extrep = replace_na(extrep, "MiSeq")) %>%
  ggplot(aes(expected.prop, observed.prop, fill = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_point(alpha=0.7, shape=21, colour="black", ) +
  geom_text(aes(x=0, y=0.9, label=paste0("RMSE: ",round(RMSE, 2))),check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #geom_text(aes(x=-10, y=1, label=paste0("A.dist: ",round(Adist, 2))),check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  scale_fill_brewer(palette="Paired")+
  facet_grid(extrep~amp_rep)+
  scale_x_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_y_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  #scale_x_continuous(labels=scales::percent)+
  #scale_y_continuous(labels = scales::percent)+
   # coord_fixed() + 
  base_theme+
  theme(legend.position="right")+
  labs(fill = "Taxon",
       x="Expected proportions",
       y="Observed proportions") 

gg.error_props

# Deviation in taxon specific errors between samples
error_variation <- joint %>% 
  filter(fcid == "HLVKYDMXX")%>%
  group_by(taxon)%>%
  group_modify(~{
    amp_err <- .x %>%
      group_by(extract_id) %>%
      add_tally()  %>%
      filter(n > 1) %>%
      summarise(error_prop = mean(error_prop)) %>%
      mutate(type = "PCR")%>%
      dplyr::select(-extract_id)
    
    ext_err  <- .x %>%
      group_by(extract_id,sample_name, fcid) %>%
      summarise(error_prop = mean(error_prop)) %>%
      ungroup() %>%
      mutate(sample_fcid = paste0(fcid, sample_name)) %>%
      group_by(sample_fcid) %>% 
      add_tally()  %>%
      filter(n > 1) %>%
      summarise(error_prop = mean(error_prop)) %>%
      mutate(type = "Extraction") %>%
      dplyr::select(-sample_fcid)
    
    samp_err  <- .x %>%
      mutate(sample_fcid = paste0(fcid, sample_name)) %>%
      group_by(sample_fcid) %>% 
      summarise(error_prop = mean(error_prop)) %>%
      ungroup() %>%
      mutate(type = "Sample") %>%
      dplyr::select(-sample_fcid)
    
    out <- bind_rows(amp_err, ext_err, samp_err)
    return(out)
  })

gg.err_var <- error_variation %>%
  ggplot(aes(x = type, y = error_prop, colour=taxon,fill=taxon, group=type))+
  geom_boxplot(fill=NA, outlier.colour = NA)+ 
  geom_point(alpha=0.7, shape=21, colour="black",position = position_jitter(width=0.2) ) +
  scale_color_brewer(palette = "Paired")+
  scale_fill_brewer(palette = "Paired")+
  facet_grid(~taxon) +
  scale_y_continuous(trans=pseudo_log_trans(1e-1))+
  base_theme +
  theme(
    strip.text = element_blank(),
    legend.position = "none"
  )+
  labs(y="Error in Proportions",
       x = NULL)


Fig2 <- gg.error_props /gg.err_var + plot_layout(heights = c(3,1)) + plot_annotation(tag_levels = "A")

Fig2

#Save figure
pdf(file="fig/fig2_error_consistency.pdf", width = 10, height = 8 , paper="a4r")
  plot(Fig2)
try(dev.off(), silent=TRUE)

```


## Merge replicates 

```{r merge replicates}
# Merge replicates
ps.merged <- ps2 %>%
    merge_samples(group = "sample_name", fun="sum")

#This loses the sample metadata - Need to add it agian
sample_data(ps.merged) <- sample_data(ps2) %>%
  as("data.frame") %>%
  dplyr::filter(!duplicated(sample_name)) %>%
  magrittr::set_rownames(.$sample_name)

ps3 <- ps.merged
```


# Get expected and observed across all runs


```{r make joint}
ps_bias <- ps3

#plot expected
exp %>% 
  group_by(sample_name) %>%
  mutate_at(vars(expected), ~ . / sum(.) ) %>% #Convert to proportions
  ggplot(aes(x=sample_name, y=expected, fill=taxon)) +
  geom_col(position="stack") + 
  scale_fill_brewer(palette="Spectral") +
  theme(legend.position = "bottom") +
  base_theme+
  labs(x = "Sample Name", y= "Expected relative abundance")

#Get obsered
sam <- speedyseq::psmelt(ps_bias) %>%
  janitor::clean_names() %>%
  filter(!str_detect(species, "__")) %>%#Remove unclassified
  mutate(taxon = species %>% str_replace(" ", "_")) %>%
  dplyr::select(sample_name,sample_id, taxon, abundance, pcr_primers, fcid, material_type = type) %>%
  mutate(sample_name = sample_name %>%
           str_remove_all("BF1-BR1-|SauronS878-HexCOIR4-|fwhF2-HexCOIR4-|fwhF2-fwhR2n-") %>%
           str_remove("HLVKYDMXX_") %>%
           str_remove("CB3DR_"),
         sample_id = paste0(fcid, "_", sample_name), #Remove this for ps1
         sample_name = sample_name %>%
           str_remove("-ex[0-9]")) 

#Join tables 
joint <- sam %>%
  filter(taxon %in% exp$taxon,
         sample_name %in% exp$sample_name) %>%
  left_join(exp, by = c("sample_name","taxon")) %>%
  dplyr::rename(observed = abundance) %>%
  filter(!is.na(expected))%>%
  mutate(expected = replace_na(expected, 0),
         observed0 = (observed + 0.5) * (expected > 0),
         error = observed0 / expected,
         #error_centre = center_elts(observed0 / expected, na.rm = TRUE),
         ) %>%
  distinct() %>%
  group_by(sample_id) %>%
  mutate(observed.prop=observed0, expected.prop=expected) %>%
  mutate_at(vars(observed.prop,  expected.prop), ~ . / sum(.) ) %>% #Convert to proportions
  mutate(sample_size = sum(expected), # Do the same for the ratios?
         observed_abs = observed.prop * sample_size,
         error_abs = observed_abs / expected,
         error_prop = observed.prop / expected.prop,
         #error_abs_centre = center_elts(observed_abs / expected, na.rm = TRUE)
         )  %>%
  group_by(sample_id) %>%
  group_modify(~{
    alr_denom <- .x %>% 
      filter(taxon == "Carpophilus_hemipterus") %>% # Set to consistent taxon across all
      pull(error)
    clr_denom <- .x %>%
      pull(error) %>%
      gm_mean()
    .x %>%
      mutate(error_alr = log(error / alr_denom),
             error_clr = log(error / clr_denom))
  }) %>%
  group_modify(~{
    alr_denom <- .x %>% 
      filter(taxon == "Carpophilus_hemipterus") %>% # Set to consistent taxon across all
      pull(error_abs)
    clr_denom <- .x %>%
      pull(error_abs) %>%
      gm_mean()
    .x %>%
      mutate(error_abs_alr = log(error_abs / alr_denom),
             error_abs_clr = log(error_abs / clr_denom))
  })%>%
  filter(!sample_name=="CML6") #Bad sample

```


# Visualise errors

```{r visualise errors}
# Original errors
error_rmse <- joint %>%
  group_by(material_type) %>%
  rmse(truth = expected.prop, estimate = observed.prop) %>%
  dplyr::select(RMSE = .estimate, material_type)

error_anorm <- joint %>%
  group_by(material_type) %>%
    summarise(Adist = anorm(observed.prop / expected)) 

## Visualise errors in proportions
gg.error_props <- joint %>% 
  filter(expected > 0) %>%
  #mutate_at(vars(expected.prop, observed.prop), logit) %>%
  left_join(error_rmse) %>%
  left_join(error_anorm) %>%
  ggplot(aes(expected.prop, observed.prop, fill = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_jitter(alpha=0.7, shape=21, colour="black", ) +
  geom_text(aes(x=-10, y=3, label=paste0("RMSE: ",round(RMSE, 2))),check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  geom_text(aes(x=-10, y=1, label=paste0("A.dist: ",round(Adist, 2))),check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  scale_fill_brewer(palette="Paired")+
    scale_x_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_y_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  facet_grid(fcid~material_type)+
    coord_fixed() + 
  theme_bw()+
  labs(fill = "Taxon",
       x="Expected proportions (log-odds)",
       y="Observed proportions (log-odds)") 

gg.error_props

## Visualise errors in abs
gg.error_abs <- joint %>% 
  filter(expected > 0) %>%
  ggplot(aes(expected, observed_abs, color = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_jitter(alpha=0.7) +
  scale_color_brewer(palette="Spectral")+
  facet_grid(material_type~fcid)+
  theme_bw()+
  labs(title = "Observed vs. expected absolute abundances",
       color = "Taxon",
       x="Expected Individuals",
       y="Observed Individuals") 

# Visualise the error in all pairwise ratios
gg.error_rat <- joint %>%
  filter(expected > 0) %>%
  dplyr::mutate(Taxon = taxon %>%
                  str_replace("^.+_", paste0(str_extract(taxon, "."), "_"))) %>% #Shorten genus names
  metacal::compute_ratios(group_vars = c("sample_id", "fcid", "material_type")) %>%
  filter(!is.na(expected)) %>%
  filter(!Taxon.x == Taxon.y) %>%
  mutate(pair = paste(Taxon.x, Taxon.y, sep = ":")) %>% 
  ggplot(aes(pair, error, colour=Taxon.x)) +
  geom_jitter(alpha=0.7) +
  geom_hline(yintercept = 1) +
  scale_y_log10() +
  facet_grid(material_type~fcid)+
  theme_bw()+
    scale_color_brewer(palette="Spectral")+
    theme(legend.position = "none",
          axis.text.x = element_text(angle=45, hjust=1)) +
  labs(x = "Taxon ratios",
       y = "Error in taxon ratios")


#Save figure
pdf(file="fig/errors.pdf", width = 11, height = 8 , paper="a4r")
  plot(gg.error_props)
  plot(gg.error_abs)
  plot(gg.error_rat)
try(dev.off(), silent=TRUE)
```

# Compare bias models


## Split data into training and testing

```{r splits}
set.seed(666)

# Generate a list of 100 seeds

seeds <- sample(1:10000, 100,replace = FALSE)

split_list <- vector("list", length=length(seeds))
for (s in 1:length(seeds)){

  set.seed(seeds[s])
  #Sample splits
  joint_split <- joint %>%
    ungroup() %>%
    dplyr::select(sample_name, material_type) %>%
    distinct() %>%
    initial_split(strata = material_type)
  
  # Training set
  joint_train <- joint %>%
    dplyr::select(-error) %>%
    #dplyr::rename(error_rat = error) %>%
    filter(sample_name %in% training(joint_split)$sample_name) %>%
    ungroup()%>%
    pivot_longer(starts_with("error"),
                 names_to="error_type",
                 values_to="error") %>%
    mutate(observed.prop =case_when(
      error_type == "error_prop" ~ logit(observed.prop),
      error_type == "error_abs" ~ log(observed_abs),
      TRUE ~ observed.prop
      ),
      expected.prop = case_when(
      error_type == "error_prop" ~ logit(expected.prop),
      error_type == "error_abs" ~ log(expected),
      TRUE ~ expected.prop
      ))
  
  # Testing set
  joint_test <- joint %>%
    dplyr::select(-error) %>%
    #dplyr::rename(error_rat = error) %>%
    filter(sample_name %in% testing(joint_split)$sample_name)%>%
    ungroup()%>%
    pivot_longer(starts_with("error"),
                 names_to="error_type",
                 values_to="error")%>%
    mutate(observed.prop =case_when(
      error_type == "error_prop" ~ logit(observed.prop),
      error_type == "error_abs" ~ log(observed_abs),
      TRUE ~ observed.prop
      ),
      expected.prop = case_when(
      error_type == "error_prop" ~ logit(expected.prop),
      error_type == "error_abs" ~ log(expected),
      TRUE ~ expected.prop
      ))
  
  # Check for data bleeding
  table(joint_train$sample_name %in% joint_test$sample_name)
  table(joint_test$sample_name %in% joint_train$sample_name)
  
  # Create Cross validation folds for model tuning 
  train_cv <- joint_train %>%
      mutate(strata = paste0(taxon, "_", material_type)) %>%
      vfold_cv(v = 10, strata = strata )

  
  
  split_list[[s]] <- tibble(seed = seeds[s],
                            train = list(joint_train),
                            test = list(joint_test),
                            cv= list(train_cv))
}

splits <- split_list %>%
  bind_rows()

```

## Uncorrected errors 

```{r uncorrected errors}
uncorrected_results_train <- joint_train %>%
  mutate(
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = observed.prop,
  calibrated = observed.prop,
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)


uncorrected_results_test <- joint_test %>%
  mutate(
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = observed.prop,
  calibrated = observed.prop,
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

```

## Preprocessing recipes

```{r preprocess}
# Recipe for fitting model including fcid + material_type
bias_recipe <- 
  recipe(formula = error ~ 0 + taxon + fcid + material_type, data = joint_train) %>% 
  step_string2factor(fcid, material_type, taxon) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot=TRUE) %>% 
  step_zv(all_predictors())
  #step_normalize(-all_nominal()) 

# Recipe for fitting separate models to fcid + material_type. 
bias_recipe_separate <- 
  recipe(formula = error ~ 0 + taxon, data = joint_train) %>% 
  #step_string2factor(one_of(fcid, material_type)) %>% 
  step_string2factor(taxon) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot=TRUE) %>% 
  step_zv(all_predictors()) #%>% 
  #step_normalize(-all_nominal()) 

#Recipe for fitting model to proportions
prop_recipe <- recipe(formula =  expected.prop ~ 0 + observed.prop + taxon + fcid + material_type, data = joint_train) %>% 
  #step_string2factor(one_of(fcid, material_type)) %>% 
  step_string2factor(taxon) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot=TRUE) %>% 
  step_zv(all_predictors())  %>%
  step_naomit(all_predictors())

prop_recipe_separate <- 
  recipe(formula = expected.prop ~ 0 + observed.prop + taxon, data = joint_train) %>% 
  #step_string2factor(one_of(fcid, material_type)) %>% 
  step_string2factor(taxon) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot=TRUE) %>% 
  step_zv(all_predictors())

```


## Linear regression
Estimating bias is complicated by the compositional nature of metabarcoding measurements. Because only relative abundances are measured, the measurement of a sample (s) only provides information about the efficiencies of the taxa in the sample relative to each other.

Here we fit a simple linear model on the compositional difference between Observed and Expected counts. The compositional error vectors are first geometrically centered, then taxon is used as a predictor.

```{r linear regression}
lm_spec <- 
  linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

#Ratios workflow
lm_workflow_rat <- 
  workflow() %>% 
  add_recipe(bias_recipe) %>% 
  add_model(lm_spec)

# Proportions workflow
lm_workflow_prop <- 
  workflow() %>% 
  add_recipe(prop_recipe) %>% 
  add_model(lm_spec)


# TESTING NEw
fit_lm <- joint_train %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  left_join(group_keys(.) %>%
            tidyr::unite("group_names", everything(), sep="-", remove=FALSE)) %>%
  mutate(
    model_obj = purrr::map2(train, error_type, function(x,y ){
      if (!y %in% c("error_prop", "error_abs")){
        fit(lm_workflow_rat, data=x)
      } else if(error_type %in% c("error_prop", "error_abs")){
        fit(lm_workflow_prop, data=x)
      }
    }),
    pred_train = purrr::map2(model_obj, train, ~safe_predict(.x, .y)),
    pred_test = purrr::map2(model_obj, test, ~safe_predict(.x, .y)),
    vip = purrr::map2(model_obj, group_names ,~{
      if(!is(.x, "data.frame")){
        .x %>%
        pull_workflow_fit() %>%
        vip::vip()+
        labs(title=.y %>% str_remove("error_"))
      } else{
        return(NULL)
      }
    })
  )





# Fit model to ratios and proportions
fit_lm <- joint_train %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  left_join(group_keys(.) %>%
            tidyr::unite("group_names", everything(), sep="-", remove=FALSE)) %>%
  mutate(
    model_obj = purrr::map2(train, error_type, function(x,y ){
      if (!y %in% c("error_prop", "error_abs")){
        fit(lm_workflow_rat, data=x)
      } else if(error_type %in% c("error_prop", "error_abs")){
        fit(lm_workflow_prop, data=x)
      }
    }),
    pred_train = purrr::map2(model_obj, train, ~safe_predict(.x, .y)),
    pred_test = purrr::map2(model_obj, test, ~safe_predict(.x, .y)),
    vip = purrr::map2(model_obj, group_names ,~{
      if(!is(.x, "data.frame")){
        .x %>%
        pull_workflow_fit() %>%
        vip::vip()+
        labs(title=.y %>% str_remove("error_"))
      } else{
        return(NULL)
      }
    })
  )

# Plot model variable importance
wrap_plots(fit_lm %>%
             pull(vip))

#results
lm_results_train <- fit_lm %>%
  unnest(pred_train, train) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0) 

lm_results_train %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

# Results test set
lm_results_test <- fit_lm %>%
  unnest(pred_test, test) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

lm_results_test %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

lm_results_train %>%
  ggplot(aes(logit(predicted), logit(observed.prop), fill = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_jitter(alpha=0.7, shape=21, colour="black", ) +
  scale_fill_brewer(palette="Paired")+
  facet_grid(error_type~material_type)+
  theme_bw()+
  labs(fill = "Taxon",
       x="Predicted proportions (log-odds)",
       y="Observed proportions (log-odds)") 
```


## LASSO

```{r LASSO}
lasso_spec <- 
    linear_reg(penalty = tune(), mixture = tune()) %>% 
    set_mode("regression") %>% 
    set_engine("glmnet") %>%
    set_args(family="gaussian")
  
# Tune ratio workflow
lasso_workflow_rat <- 
    workflow() %>% 
    add_recipe(bias_recipe) %>% 
    add_model(lasso_spec) 
  
lasso_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, 
    0.2, 0.4, 0.6, 0.8, 1)) 

lasso_tune <- tune_grid(lasso_workflow_rat, resamples = train_cv, grid = lasso_grid)

autoplot(lasso_tune , metric = "rmse") +
  scale_x_log10()

lasso_workflow_rat <- finalize_workflow(lasso_workflow_rat, lasso_tune %>%
  select_best("rmse"))

# Tune proporiton workflow
lasso_workflow_prop <- 
    workflow() %>% 
    add_recipe(prop_recipe) %>% 
    add_model(lasso_spec) 
  
lasso_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, 
    0.2, 0.4, 0.6, 0.8, 1)) 

lasso_tune <- tune_grid(lasso_workflow_prop, resamples = train_cv, grid = lasso_grid)

autoplot(lasso_tune , metric = "rmse") +
  scale_x_log10()

lasso_workflow_prop <- finalize_workflow(lasso_workflow_prop, lasso_tune %>%
  select_best("rmse"))

fit_lasso <- joint_train %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  left_join(group_keys(.) %>%
            tidyr::unite("group_names", everything(), sep="-", remove=FALSE)) %>%
  mutate(
    model_obj = purrr::map2(train, error_type, function(x,y ){
      if (!y  %in% c("error_prop", "error_abs")){
        fit(lasso_workflow_rat, data=x)
      } else if(error_type  %in% c("error_prop", "error_abs")){
        fit(lasso_workflow_prop, data=x)
      }
    }),
    pred_train = purrr::map2(model_obj, train, ~safe_predict(.x, .y)),
    pred_test = purrr::map2(model_obj, test, ~safe_predict(.x, .y)),
    vip = purrr::map2(model_obj, group_names,~{
      if(!is(.x, "data.frame")){
        .x %>%
        pull_workflow_fit() %>%
        vip::vip()+
        labs(title=.y %>% str_remove("error_"))
      } else{
        return(NULL)
      }
      
    })
  )

# Plot model variable importance
wrap_plots(fit_lasso$vip)

#results
lasso_results_train <- fit_lasso %>%
  unnest(pred_train, train) %>%
  dplyr::select(!where(is.list))%>%
  ungroup() %>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  mutate(residual = calibrated / expected.prop) %>%
  dplyr::filter(expected > 0)

lasso_results_train %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

# Results test set
lasso_results_test <- fit_lasso %>%
  unnest(pred_test, test) %>% 
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

lasso_results_test %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

lasso_results_test %>%
  ggplot(aes(logit(predicted), logit(observed.prop), fill = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_jitter(alpha=0.7, shape=21, colour="black", ) +
  scale_fill_brewer(palette="Paired")+
  facet_grid(error_type~material_type)+
  theme_bw()+
  labs(fill = "Taxon",
       x="Predicted proportions (log-odds)",
       y="Observed proportions (log-odds)") 
```

## Random Forest

```{r RF}
ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("regression") %>% 
  set_engine("ranger", importance = "impurity") 

#Ratio workflow
ranger_workflow_rat <- 
  workflow() %>% 
  add_recipe(bias_recipe) %>% 
  add_model(ranger_spec) 

doParallel::registerDoParallel(cores=2)
set.seed(666)
ranger_tune <- tune_grid(ranger_workflow_rat, resamples = train_cv, grid = 20)

autoplot(ranger_tune , metric = "rmse") 

ranger_workflow_rat <- finalize_workflow(ranger_workflow_rat, ranger_tune %>%
  select_best("rmse"))

#Proporiton workflow
ranger_workflow_prop <- 
  workflow() %>% 
  add_recipe(prop_recipe) %>% 
  add_model(ranger_spec) 

set.seed(666)
ranger_tune <- tune_grid(ranger_workflow_prop, resamples = train_cv, grid = 20)

autoplot(ranger_tune , metric = "rmse") 

ranger_workflow_prop <- finalize_workflow(ranger_workflow_prop, ranger_tune %>%
  select_best("rmse"))

fit_rf <- joint_train %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  left_join(group_keys(.) %>%
            tidyr::unite("group_names", everything(), sep="-", remove=FALSE)) %>%
  mutate(
    model_obj = purrr::map2(train, error_type, function(x,y ){
      if (!y  %in% c("error_prop", "error_abs")){
        fit(ranger_workflow_rat, data=x)
      } else if(error_type  %in% c("error_prop", "error_abs")){
        fit(ranger_workflow_prop, data=x)
      }
    }),
    pred_train = purrr::map2(model_obj, train, ~safe_predict(.x, .y)),
    pred_test = purrr::map2(model_obj, test, ~safe_predict(.x, .y)),
    vip = purrr::map2(model_obj, group_names,~{
      if(!is(.x, "data.frame")){
        .x %>%
        pull_workflow_fit() %>%
        vip::vip()+
        labs(title=.y %>% str_remove("error_"))
      } else{
        return(NULL)
      }
      
    })
  )

# Plot model variable importance
wrap_plots(fit_rf$vip)

#results
rf_results_train <- fit_rf %>%
  unnest(pred_train, train) %>% 
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

rf_results_train %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

# Results test set
rf_results_test <- fit_rf %>%
  unnest(pred_test, test) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

rf_results_test %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

rf_results_test %>%
  ggplot(aes(logit(predicted), logit(observed.prop), fill = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_jitter(alpha=0.7, shape=21, colour="black", ) +
  scale_fill_brewer(palette="Paired")+
  facet_grid(error_type~material_type)+
  theme_bw()+
  labs(fill = "Taxon",
       x="Predicted proportions (log-odds)",
       y="Observed proportions (log-odds)") 

```


## XGBOOST

```{r XGBOOST}
xgboost_spec <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
    loss_reduction = tune(), sample_size = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost") 

# Ratio workflow
xgboost_workflow_rat <- workflow() %>% 
  add_recipe(bias_recipe) %>% 
  add_model(xgboost_spec) 

doParallel::registerDoParallel(cores=2)
set.seed(666)
xgboost_tune <-
  tune_grid(xgboost_workflow_rat, resamples = train_cv, grid =20)

autoplot(xgboost_tune , metric = "rmse") 

xgboost_workflow_rat <- finalize_workflow(xgboost_workflow_rat, xgboost_tune %>%
  select_best("rmse"))

# Proportion workflow
xgboost_workflow_prop <- workflow() %>% 
  add_recipe(prop_recipe) %>% 
  add_model(xgboost_spec) 

doParallel::registerDoParallel(cores=2)
set.seed(666)
xgboost_tune <- tune_grid(xgboost_workflow_prop, resamples = train_cv, grid =20)

autoplot(xgboost_tune , metric = "rmse") 

xgboost_workflow_prop <- finalize_workflow(xgboost_workflow_prop, xgboost_tune %>%
  select_best("rmse"))

fit_xg <- joint_train %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  left_join(group_keys(.) %>%
            tidyr::unite("group_names", everything(), sep="-", remove=FALSE)) %>%
  mutate(
    model_obj = purrr::map2(train, error_type, function(x,y ){
      if (!y %in%  c("error_prop", "error_abs") ){
        fit(xgboost_workflow_rat, data=x)
      } else if(error_type %in%  c("error_prop", "error_abs") ){
        fit(xgboost_workflow_prop, data=x)
      }
    }),
    pred_train = purrr::map2(model_obj, train, ~safe_predict(.x, .y)),
    pred_test = purrr::map2(model_obj, test, ~safe_predict(.x, .y)),
    vip = purrr::map2(model_obj, group_names,~{
      if(!is(.x, "data.frame")){
        .x %>%
        pull_workflow_fit() %>%
        vip::vip()+
        labs(title=.y %>% str_remove("error_"))
      } else{
        return(NULL)
      }
      
    })
  )

# Plot model variable importance
wrap_plots(fit_xg$vip)

#results
xg_results_train <- fit_xg %>%
  unnest(pred_train, train) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

xg_results_train %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

# Results test set
xg_results_test <- fit_xg %>%
  unnest(pred_test, test) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

xg_results_test %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

xg_results_test %>%
  ggplot(aes(logit(predicted), logit(observed.prop), fill = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_jitter(alpha=0.7, shape=21, colour="black", ) +
  scale_fill_brewer(palette="Paired")+
  facet_grid(error_type~material_type)+
  theme_bw()+
  labs(fill = "Taxon",
       x="Predicted proportions (log-odds)",
       y="Observed proportions (log-odds)") 

```


## SVM

```{r SVM}
svm_spec <- 
  svm_poly(cost = tune(), degree = tune(), scale_factor = tune(), margin = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("kernlab") 

# Ratio workflow
svm_workflow_rat <- workflow() %>% 
  add_recipe(bias_recipe) %>% 
  add_model(svm_spec) 

#doParallel::registerDoParallel(cores=2)
set.seed(666)
svm_tune <-
  tune_grid(svm_workflow_rat, resamples = train_cv, grid =20)

autoplot(svm_tune , metric = "rmse") 

svm_workflow_rat <- finalize_workflow(xgboost_workflow_rat, svm_tune %>%
  select_best("rmse"))

# Proportion workflow
svm_workflow_prop <- workflow() %>% 
  add_recipe(prop_recipe) %>% 
  add_model(svm_spec) 

doParallel::registerDoParallel(cores=2)
set.seed(666)
svm_tune <- tune_grid(svm_workflow_prop, resamples = train_cv, grid =20)

autoplot(svm_tune , metric = "rmse") 

svm_workflow_prop <- finalize_workflow(svm_workflow_prop, svm_tune %>%
  select_best("rmse"))

fit_svm <- joint_train %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  left_join(group_keys(.) %>%
            tidyr::unite("group_names", everything(), sep="-", remove=FALSE)) %>%
  mutate(
    model_obj = purrr::map2(train, error_type, function(x,y ){
      if (!y %in%  c("error_prop", "error_abs") ){
        fit(xgboost_workflow_rat, data=x)
      } else if(error_type %in%  c("error_prop", "error_abs") ){
        fit(xgboost_workflow_prop, data=x)
      }
    }),
    pred_train = purrr::map2(model_obj, train, ~safe_predict(.x, .y)),
    pred_test = purrr::map2(model_obj, test, ~safe_predict(.x, .y)),
    vip = purrr::map2(model_obj, group_names,~{
      if(!is(.x, "data.frame")){
        .x %>%
        pull_workflow_fit() %>%
        vip::vip()+
        labs(title=.y %>% str_remove("error_"))
      } else{
        return(NULL)
      }
      
    })
  )

# Plot model variable importance
wrap_plots(fit_svm$vip)

#results
svm_results_train <- fit_svm %>%
  unnest(pred_train, train) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

svm_results_train %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

# Results test set
svm_results_test <- fit_svm %>%
  unnest(pred_test, test) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

svm_results_test %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

svm_results_test %>%
  ggplot(aes(logit(predicted), logit(observed.prop), fill = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_jitter(alpha=0.7, shape=21, colour="black", ) +
  scale_fill_brewer(palette="Paired")+
  facet_grid(error_type~material_type)+
  theme_bw()+
  labs(fill = "Taxon",
       x="Predicted proportions (log-odds)",
       y="Observed proportions (log-odds)") 

```

## Neural Network

These models dont seem to be estimating taxon specific slopes??? maybe biased for some

```{r Neural network}
keras_spec <- linear_reg(mixture=tune(),penalty = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("keras",verbose = 0) 

# Ratio workflow
keras_workflow_rat <- 
  workflow() %>% 
  add_recipe(bias_recipe) %>% 
  add_model(keras_spec) 

set.seed(666)
keras_tune <- tune_grid(keras_workflow_rat, resamples = train_cv, grid =20)

autoplot(keras_tune , metric = "rmse") 

keras_workflow_rat <- finalize_workflow(keras_workflow_rat, keras_tune%>%
  select_best("rmse"))

# Proportions workflow
keras_workflow_prop <- 
  workflow() %>% 
  add_recipe(prop_recipe) %>% 
  add_model(keras_spec) 

set.seed(666)
keras_tune <- tune_grid(keras_workflow_prop, resamples = train_cv, grid =20)

autoplot(keras_tune , metric = "rmse") 

keras_workflow_prop <- finalize_workflow(keras_workflow_prop, keras_tune%>%
  select_best("rmse"))


fit_keras <- joint_train %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  left_join(group_keys(.) %>%
            tidyr::unite("group_names", everything(), sep="-", remove=FALSE)) %>%
  mutate(
    model_obj = purrr::map2(train, error_type, function(x,y ){
      if (!y %in%  c("error_prop", "error_abs") ){
        fit(keras_workflow_rat, data=x)
      } else if(error_type %in%  c("error_prop", "error_abs") ){
        fit(keras_workflow_prop, data=x)
      }
    }),
    pred_train = purrr::map2(model_obj, train, ~safe_predict(.x, .y)),
    pred_test = purrr::map2(model_obj, test, ~safe_predict(.x, .y))
  )

# Plot model variable importance
wrap_plots(fit_xg$vip)


#results
keras_results_train <- fit_keras %>%
  unnest(pred_train, train) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

keras_results_train %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

# Results test set
keras_results_test <- fit_keras %>%
  unnest(pred_test, test) %>%
  dplyr::select(!where(is.list))%>%
  mutate(estimated = case_when(
    error_type %in% c("error_alr", "error_clr", "error_centre", "error_abs_centre", "error_abs_alr", "error_abs_clr","error_log", "error_abs_log", "error_abs") ~ exp(.pred),
    error_type %in% c( "error_prop") ~ expit(.pred),
    TRUE ~ .pred
  ),
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
  predicted = case_when(
    error_type %in% c("error_prop", "error_abs") ~ estimated,
    TRUE ~ expected.prop * estimated),
  calibrated = case_when(
    error_type %in% c("error_abs", "error_prop") ~ estimated,
    TRUE ~ observed.prop / estimated),
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

keras_results_train %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

keras_results_test %>%
  ggplot(aes(logit(predicted), logit(observed.prop), fill = taxon)) + 
  geom_abline(intercept = 0, slope=1) +
  geom_jitter(alpha=0.7, shape=21, colour="black", ) +
  scale_fill_brewer(palette="Paired")+
  facet_grid(error_type~material_type)+
  theme_bw()+
  labs(fill = "Taxon",
       x="Predicted proportions (log-odds)",
       y="Observed proportions (log-odds)") 
```

## metacal

Here we use the bias estimation approach of *McLaren, M. R., Willis, A. D., & Callahan, B. J. (2019). Consistent and correctable bias in metagenomic sequencing experiments. Elife, 8.* This approach treats the abundances and errors as compositional vectors and estimates the bias as the geometric center of these bias vectors. 


```{r metacal}
# This is a modified version of the metacal::center function using the "proj" method
# This has been modified to auto detect the input scale and work better with the tidymodels workflow
# Original code from https://github.com/mikemc/metacal/blob/master/R/compositional-mean.R
# Method from vandenBoogaart2006; also described in vandenBoogaart2013 and at
# https://core.ac.uk/download/pdf/132548286.pdf (Bren2008)

metacal_spec <- function(x, denom="all", in_scale = NULL, out_scale="linear"){
  
  #input checks
  if(!is(x, "data.frame")){return(NULL)}    
  if(!denom %in% c("all", unique(x$taxon))){
    stop("denom must either be all, or a taxon name")
  }
  
  #Transform x to matrix
   mat <- x %>%
          dplyr::select(sample_id, taxon, error)%>%
          pivot_wider(id_cols= sample_id,
                      names_from = "taxon",
                      values_from = "error",
                      values_fill = list(error=NaN))%>%    
          column_to_rownames("sample_id") %>%
          as.matrix() 
    
   # Check if matrix is log or linear
   if(is.null(in_scale)){
     curr_nas <- sum(is.na(mat))
     new_nas <- sum(is.na(log(mat)))
     if(curr_nas == new_nas){
       in_scale <- "linear"
       warning("No input scale provided, guessed ", in_scale)
     } else if(!curr_nas == new_nas){
       in_scale <- "log"
       warning("No input scale provided, guessed ", in_scale)
     }
   }

  # Drop missing data
  if (in_scale == "linear"){
    rows_to_keep <- rowSums(mat, na.rm=TRUE) > 0
    cols_to_keep <- colSums(mat, na.rm=TRUE) > 0
    
   } else  if(in_scale=="log"){
    rows_to_keep <- rowSums(is.na(mat)) < ncol(mat)
    cols_to_keep <- colSums(is.na(mat)) < nrow(mat)
  }
   mat <- mat[rows_to_keep, cols_to_keep]
   
   # Transform to log scale
   if (in_scale == "linear"){
      mat <- log(mat)
   }
    
    # log center proj
    K <- ncol(mat)
    mat0 <- mat
    mat0[is.nan(mat0)] <- 0
    
    P_sum <- diag(0, nrow = K)
    v_sum <- rep(0, K)
    weights = rep(1, nrow(mat))
    
    proj_mat <- function(K, M = c()) {
      mat <- diag(nrow = K) - 1/(K - length(M))
      mat[M,] <- 0
      mat[,M] <- 0
      mat
    }
    
    for (i in seq(nrow(mat))) {
        M <- which(is.nan(mat[i,]))
        v <- mat0[i,]
        P <- proj_mat(K, M) * weights[i]
        P_sum <- P_sum + P
        v_sum <- v_sum + P %*% v
    }

    #CLR transform
    clr_center <- (MASS::ginv(P_sum) %*% v_sum) %>% c
    names(clr_center) <- colnames(mat)
    
    out <- clr_center

    if (!denom=="all"){
      out <- out - mean(out[denom])
    }
    if (out_scale == "linear"){
      out <- exp(out)
    }
    out <- tibble::enframe(out, "taxon", ".pred")    
}

## Tests to make sure that the pre transformed errors from earlier are equivalent
## They should be because everything we are looking at is a ratio
tests <- joint %>%
  dplyr::rename(error_rat = error) %>%
  filter(sample_id %in% training(joint_split)$sample_id) %>%
  ungroup()%>%
  pivot_longer(starts_with("error"),
               names_to="error_type",
               values_to="error") %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest()

# Test that ALR on original errors is equivalent to the pre-transformed ALR error
test1 <- metacal_spec((tests %>% filter(error_type == "error_rat"))$data[[1]],
                      denom="Carpophilus_hemipterus")
test2 <- metacal_spec((tests %>% filter(error_type == "error_alr"))$data[[1]],
                      denom="Carpophilus_hemipterus")
all(round(test1$.pred,4) == round(test2$.pred,4))


#Test CLR on on original erorrs is equivalent to the pre-transformed CLR error
test3 <- metacal_spec((tests %>% filter(error_type == "error_rat"))$data[[1]])
test4 <- metacal_spec((tests %>% filter(error_type == "error_clr"))$data[[1]])
all(round(test3$.pred,4) == round(test4$.pred,4))

# Fit metacal
fit_metacal <- joint_train %>%
  filter(expected > 0) %>%
  #filter(!error_type == "error_prop") %>%
  group_by(error_type, fcid, material_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
    filter(expected > 0) %>%
  group_by(error_type, fcid, material_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  mutate(denom = case_when(
    error_type == "error_alr"  ~ "Carpophilus_hemipterus",
    TRUE ~ "all"
  )) %>%
   mutate(fits = purrr::map(train, metacal_spec, denom=denom))

# Testing isnt working right

# Results
metacal_results_train <-  fit_metacal %>%
  unnest(train) %>%
  left_join(fit_metacal %>% 
              unnest(fits) %>% 
              dplyr::select(error_type,taxon, .pred))%>% #material_type, fcid
  dplyr::filter(expected > 0) %>%
  mutate(
  observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
    estimated = .pred,
    predicted = expected.prop * estimated,
    calibrated = observed.prop / estimated,
    residual = observed.prop / predicted
    )  %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  dplyr::filter(expected > 0)%>% 
    dplyr::select(!where(is.list))

metacal_results_train %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

# Results test set
metacal_results_test <- fit_metacal%>%
  unnest(test) %>%
  left_join(fit_metacal %>% 
              unnest(fits) %>% 
              dplyr::select(error_type, taxon,  .pred)) %>% #material_type, fcid, 
  dplyr::filter(expected > 0) %>%
  mutate(
    observed.prop = case_when(
    error_type %in% c("error_abs") ~ exp(observed.prop),
    error_type %in% c("error_prop") ~ expit(observed.prop),
    TRUE ~ observed.prop),
  expected.prop = case_when(
    error_type %in% c("error_abs") ~ exp(expected.prop),
    error_type %in% c("error_prop") ~ expit(expected.prop),
    TRUE ~ expected.prop),
    estimated = .pred,
    predicted = expected.prop * estimated,
    calibrated = observed.prop / estimated,
    residual = observed.prop / predicted
    )  %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  dplyr::filter(expected > 0)%>% 
    dplyr::select(!where(is.list))

metacal_results_test %>%
  group_by(error_type) %>%
  rmse(truth = observed.prop, estimate = predicted)

```


# Evaluate all model fits

Here we compare the accuracy of the bias estimation procedure by looking at the Root Mean Square Error (RMSE) between the observed relative abundances from sequencing, and the predicted relative abundances (Expected * Bias estimate) from each model.

To determine the models predictive ability to new data, we also determine these metrics for a seperate testing set of samples that the model was not trained on.


## RA preds

Add Adist
Add original bias?

```{r evaluate models}
all_fits_train <- do.call("list",mget(grep("results_train",names(.GlobalEnv),value=TRUE))) %>%
  bind_rows(.id="model") %>%
  distinct() %>%
  filter(!str_detect(model, "final"))%>%
  filter(!str_detect(model, "keras"))

all_fits_test <- do.call("list",  mget(grep("results_test",names(.GlobalEnv),value=TRUE))) %>%
  bind_rows(.id="model") %>%
  distinct() %>%
  filter(!str_detect(model, "final"))%>%
  filter(!str_detect(model, "keras"))

fits <- all_fits_train %>%
  mutate(train = "Training set") %>%
  bind_rows(all_fits_test %>%
    mutate(train = "Test set")) %>%
  mutate(model = model %>%
           str_remove("_results.*$") %>%
           str_remove("NA")) %>%
  dplyr::select(model, taxon, material_type, sample_name, sample_id, fcid, expected, expected.prop ,observed, observed.prop, predicted, estimated, train, calibrated, residual,  error_type ,sample_size) %>%
  mutate(model = model %>%
           str_replace("keras", "Neural Network") %>%
           str_replace("rf", "Random Forest") %>%
           str_replace("^lm$", "Linear Regression") %>%
           str_replace("xg", "XGBOOST Tree") %>%
           str_replace("lasso", "LASSO Regression") %>%
           str_replace("uncorrected", "Uncorrected")%>%
           str_replace("svm", "SVM"))

# Get RMSE
model_rmse <- fits %>%
  group_by(model, error_type,  train) %>%
  rmse(truth = calibrated, estimate = expected.prop) %>%
  dplyr::select(RMSE = .estimate, model, error_type, train)

model_anorm <- fits %>%
  group_by(model, error_type, train) %>%
    summarise(Adist = anorm(calibrated / expected.prop)) 

# Visualise fits to data
gg.train_preds <- fits %>%
  left_join(model_rmse) %>%
  left_join(model_anorm) %>%
  distinct() %>%
  filter(train == "Training set") %>%
  filter(error_type %in% c("error_prop", "error_clr", "error_alr")) %>% #"error_abs", 
  mutate(error_type = error_type %>% 
           str_replace("error_prop", "Proportions") %>%
           str_replace("error_clr", "CLR Transform")%>%
           str_replace("error_alr", "ALR Transform")) %>%
  mutate(error_type = factor(error_type, 
                             levels = c("Proportions", "CLR Transform", "ALR Transform"))) %>%
  mutate(model = factor(model),
         model = fct_reorder(model, -RMSE)) %>%
  ggplot(aes(calibrated, expected.prop, fill = taxon)) +
  geom_point(alpha = 0.5, shape=21, colour="black") +
  geom_abline(lty = 2, colour = "gray80", size = 1) +
  geom_text(aes(x=0, y=.9, label=paste0("RMSE: ",round(RMSE, 2))),
            check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #geom_text(aes(x=-10, y=1, label=paste0("A.dist: ",round(Adist, 2))),
  #check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #coord_fixed() + 
  facet_grid(error_type~model) +
  base_theme+
  scale_x_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent,
                       breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_y_continuous(trans = scales::pseudo_log_trans(1e-3),
                     labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_fill_brewer(palette="Paired") +
  theme(panel.grid = element_line(size = rel(1)),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "bottom",
        legend.text = element_text(face="italic")) +
    labs(x = "Calibrated proportions", 
        y = "Expected proportions",
        fill = "Sample type", 
        colour= NULL,
        title= "Training set")

gg.train_preds

# Visualise fits to data
gg.test_preds <- fits %>%
  left_join(model_rmse) %>%
  left_join(model_anorm) %>%
  filter(train == "Test set") %>%
  filter(error_type %in% c("error_prop", "error_clr", "error_alr")) %>% #"error_abs", 
  mutate(error_type = error_type %>% 
           str_replace("error_prop", "Proportions") %>%
           str_replace("error_clr", "CLR Transform")%>%
           str_replace("error_alr", "ALR Transform")) %>%
  mutate(error_type = factor(error_type, 
                             levels = c("Proportions", "CLR Transform", "ALR Transform"))) %>%
  mutate(model = factor(model),
         model = fct_reorder(model, -RMSE)) %>%
  ggplot(aes(calibrated, expected.prop, fill = taxon)) +
  geom_point(alpha = 0.5, shape=21, colour="black") +
  geom_abline(lty = 2, colour = "gray80", size = 1) +
  geom_text(aes(x=0, y=.9, label=paste0("RMSE: ",round(RMSE, 2))),
              check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #geom_text(aes(x=-10, y=1, label=paste0("A.dist: ",round(Adist, 2))),
  #check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #coord_fixed() + 
  facet_grid(error_type~model) +
  base_theme+
  scale_fill_brewer(palette="Paired") +
  scale_x_continuous(trans = scales::pseudo_log_trans(1e-3),
                       labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_y_continuous(trans = scales::pseudo_log_trans(1e-3),
                     labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  theme(panel.grid = element_line(size = rel(1)),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "bottom") +
    labs(x = "Calibrated proportions ", 
        y = "Expected proportions",
        fill = "Sample type", 
        colour= NULL,
        title= "Test set")

gg.test_preds

# Predictions for just ABS across the different material types
# Visualise fits to data
gg.material_pred_train <- fits %>%
  left_join(model_rmse) %>%
  left_join(model_anorm) %>%
  distinct()%>%
  filter(train == "Training set") %>%
  filter(error_type =="error_prop") %>%
  mutate(model = factor(model),
         model = fct_reorder(model, -RMSE)) %>%
  ggplot(aes(calibrated, expected.prop, fill = material_type)) +
  geom_point(alpha = 0.5, shape=21, colour="black") +
  geom_abline(lty = 2, colour = "gray80", size = 1) +
  geom_text(aes(x=0, y=.9, label=paste0("RMSE: ",round(RMSE, 2))),
            check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #geom_text(aes(x=-10, y=1, label=paste0("A.dist: ",round(Adist, 2))),
  #check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #coord_fixed() + 
  facet_grid(material_type~model) +
  base_theme+
  scale_fill_brewer(palette="Set1") +
  scale_x_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_y_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  theme(panel.grid = element_line(size = rel(1)),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "bottom",
        legend.text = element_text(face="italic")) +
    labs(x = "Bias model prediction", 
        y = "Observed proportions",
        fill = "Taxon", 
        colour= NULL,
        title= "Training set")

# Visualise fits to data
gg.material_pred_test <- fits %>%
  left_join(model_rmse) %>%
  left_join(model_anorm) %>%
  distinct()%>%
  filter(train == "Test set") %>%
  filter(error_type =="error_prop") %>%
  mutate(model = factor(model),
         model = fct_reorder(model, -RMSE)) %>%
  ggplot(aes(calibrated, expected.prop, fill = material_type)) +
  geom_point(alpha = 0.5, shape=21, colour="black") +
  geom_abline(lty = 2, colour = "gray80", size = 1) +
  geom_text(aes(x=0, y=.9, label=paste0("RMSE: ",round(RMSE, 2))),
              check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #geom_text(aes(x=-10, y=1, label=paste0("A.dist: ",round(Adist, 2))),
  #check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  #coord_fixed() + 
  facet_grid(material_type~model) +
  base_theme+
  scale_fill_brewer(palette="Set1") +
  scale_x_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_y_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  theme(panel.grid = element_line(size = rel(1)),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "bottom",
        legend.text = element_text(face="italic")) +
    labs(x = "Bias model prediction", 
        y = "Observed proportions",
        fill = "Taxon", 
        colour= NULL,
        title= "Testing set")


#Save figure
pdf(file="fig/bias_preds.pdf", width = 11, height = 8 , paper="a4r")
  plot(gg.train_preds)
  plot(gg.test_preds)
  plot(gg.material_pred_train)
  plot(gg.material_pred_test)
try(dev.off(), silent=TRUE)

  
  
# RMSE by model, material, and error type
```

# Check for outliers

```{r outliers}
# Plot residuals by sample
fits %>%
  ggplot(aes(x  = sample_id, y = residual)) +
  geom_col() + 
  facet_grid(model~.) +
  base_theme

tmp1 <- fits %>% 
    mutate(out = abs(residual) > quantile(abs(residual), .9975)) %>% 
    dplyr::select(sample_id, material_type, fcid, expected.prop, observed.prop, predicted, residual, out, model)

ggplot(tmp1, aes(x = predicted, y = residual, colour = out, label = sample_id)) +
    geom_point(size = 2, alpha = .7, show.legend = FALSE) + 
    # geom_rug(sides = "r", outside = TRUE) +
    # coord_cartesian(clip = "off") +
    scale_color_viridis_d(end = .7, direction = -1) +
    ggrepel::geom_label_repel(
        data = filter(tmp1, out == 1), show.legend = FALSE) +
    facet_grid(model~.) +
    labs(title = "Outlier analysis")

```

## Abs preds

```{r absolute abundance predictions}
all_fits_train <- do.call("list",mget(grep("results_train",names(.GlobalEnv),value=TRUE))) %>%
  bind_rows(.id="model") %>%
  distinct() %>%
  filter(!str_detect(model, "final"))%>%
  filter(!str_detect(model, "keras"))

all_fits_test <- do.call("list",  mget(grep("results_test",names(.GlobalEnv),value=TRUE))) %>%
  bind_rows(.id="model") %>%
  distinct()%>%
  filter(!str_detect(model, "final"))%>%
  filter(!str_detect(model, "keras"))

abs_fits <- all_fits_train %>%
  mutate(train = "Training set") %>%
  bind_rows(all_fits_test %>%
    mutate(train = "Test set")) %>%
  mutate(model = model %>%
           str_remove("_results.*$") %>%
           str_remove("NA")) %>%
  dplyr::select(model, taxon, material_type, sample_name, sample_id, fcid, expected, expected.prop ,observed, observed.prop, predicted, estimated, calibrated, train, error_type ,sample_size) %>%
  mutate(predicted_abs = predicted * sample_size,
         observed_abs = observed.prop * sample_size,
         calibrated_abs = calibrated * sample_size)%>%
  mutate(model = model %>%
           str_replace("keras", "Neural Network") %>%
           str_replace("rf", "Random Forest") %>%
           str_replace("^lm$", "Linear Regression") %>%
           str_replace("xg", "XGBOOST Tree") %>%
           str_replace("lasso", "LASSO Regression") %>%
           str_replace("uncorrected", "Uncorrected")%>%
           str_replace("svm", "SVM"))

# Get RMSE
model_rmse <- abs_fits %>%
  group_by(model, error_type,  train) %>%
  rmse(truth = expected, estimate = calibrated_abs) %>%
  dplyr::select(RMSE = .estimate, model, error_type, train)

model_anorm <- abs_fits %>%
  group_by(model, error_type, train) %>%
    summarise(Adist = anorm(expected / calibrated_abs)) 

# Visualise fits to data
gg.train_abs <- abs_fits %>%
  left_join(model_rmse) %>%
  left_join(model_anorm) %>%
  distinct()%>%
  filter(train == "Training set") %>%
  filter(error_type %in% c("error_prop", "error_clr", "error_alr")) %>% #"error_abs", 
  mutate(error_type = error_type %>% 
           str_replace("error_prop", "Proportions") %>%
           str_replace("error_clr", "CLR Transform")%>%
           str_replace("error_alr", "ALR Transform")) %>%
  mutate(error_type = factor(error_type, 
                             levels = c("Proportions", "CLR Transform", "ALR Transform"))) %>%
  mutate(model = factor(model),
         model = fct_reorder(model, -RMSE)) %>%
  ggplot(aes(calibrated_abs, expected, fill = material_type)) +
  geom_point(alpha = 0.5, shape=21, colour="black") +
  geom_abline(lty = 2, colour = "gray80", size = 1) +
  geom_text(aes(x=0, y=300, label=paste0("RMSE: ",round(RMSE, 2))),check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
 # geom_text(aes(x=0, y=280, label=paste0("A.dist: ",round(Adist, 2))),check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  coord_fixed() + 
  facet_grid(error_type~model) +
    base_theme+
  scale_fill_brewer(palette="Set1") +
  theme(panel.grid = element_line(size = rel(1)),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "bottom") +
    labs(x = "Calibrated individuals", 
        y = "Expected individuals",
        fill = "Sample type", 
        colour= NULL,
        title= "Training set")

gg.train_abs

# Test set absolute
gg.test_abs <- abs_fits %>%
  left_join(model_rmse) %>%
  left_join(model_anorm) %>%
  distinct()%>%
  filter(train == "Test set") %>%
  filter(error_type %in% c("error_prop", "error_clr", "error_alr")) %>% #"error_abs", 
  mutate(error_type = error_type %>% 
           str_replace("error_prop", "Proportions") %>%
           str_replace("error_clr", "CLR Transform")%>%
           str_replace("error_alr", "ALR Transform")) %>%
  mutate(error_type = factor(error_type, 
                             levels = c("Proportions", "CLR Transform", "ALR Transform"))) %>%
  mutate(model = factor(model),
         model = fct_reorder(model, -RMSE)) %>%
  ggplot(aes(calibrated_abs, expected, fill = material_type)) +
  geom_point(alpha = 0.5, shape=21, colour="black") +
  geom_abline(lty = 2, colour = "gray80", size = 1) +
  geom_text(aes(x=0, y=300, label=paste0("RMSE: ",round(RMSE, 2))),check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
 # geom_text(aes(x=0, y=280, label=paste0("A.dist: ",round(Adist, 2))),check_overlap = TRUE, inherit.aes = FALSE, hjust = 0)+
  coord_fixed() + 
  facet_grid(error_type~model) +
    base_theme+
  scale_fill_brewer(palette="Set1") +
  theme(panel.grid = element_line(size = rel(1)),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "bottom") +
    labs(x = "Calibrated individuals", 
        y = "Expected individuals",
        fill = "Sample type", 
        colour= NULL,
        title= "Test set")

gg.test_abs

#Save figure
pdf(file="fig/abs_preds.pdf", width = 11, height = 10 , paper="a4r")
  plot(gg.train_abs)
  plot(gg.test_abs)
try(dev.off(), silent=TRUE)
```

# RMSE plot

```{r}
# Define normalised RMSE function 
# Modified from https://www.marinedatascience.co/blog/2019/01/07/normalizing-the-rmse/
nrmse_func <-  function(truth, estimate, type = "sd") {
  
    squared_sums <- sum((truth - estimate)^2)
    mse <- squared_sums/length(truth)
    rmse <- sqrt(mse)
    if (type == "sd") nrmse <- rmse/sd(truth)
    if (type == "mean") nrmse <- rmse/mean(truth)
    if (type == "maxmin") nrmse <- rmse/ (max(truth) - min(truth))
    if (type == "iq") nrmse <- rmse/ (quantile(truth, 0.75) - quantile(truth, 0.25))
    if (!type %in% c("mean", "sd", "maxmin", "iq")) message("Wrong type!")
    nrmse <- round(nrmse, 3)
 return(nrmse)
    
}

# Get RMSE
gg.rmse <- fits %>%
  mutate(train = paste0("Relative - ", train)) %>%
  group_by(model, error_type,  train) %>%
  summarise(RMSE = nrmse_func(truth = calibrated, estimate = expected.prop, type = "sd"))  %>% 
  bind_rows(
    abs_fits %>%
    mutate(train = paste0("Absolute - ", train)) %>%
    group_by(model, error_type,  train) %>%
    summarise(RMSE = nrmse_func(truth = expected, estimate = calibrated_abs, type = "sd"))) %>%
  filter(error_type %in% c("error_prop", "error_abs", "error_clr", "error_alr")) %>%
  mutate(model = factor(model, levels = c("Uncorrected", "metacal", "Linear Regression", "LASSO Regression", "SVM", "Random Forest", "XGBOOST Tree")),
         train = factor(train, levels=c("Relative - Training set", "Relative - Test set", 
                                        "Absolute - Training set",  "Absolute - Test set"))) %>%
  ggplot(aes(x = model, y=RMSE, fill=model))+
  geom_col()+
  scale_fill_brewer(palette="Paired")+
  facet_grid(train~error_type)+
  base_theme +
  labs(title = "Absolute Abundances",
       y = "Normalised RMSE",
       x = "Model Type")

gg.abs_rmse

gg.rmse <- fits %>%
  group_by(model, error_type,  train) %>%
  summarise(RMSE = nrmse_func(truth = calibrated, estimate = expected.prop, type = "sd"))  %>%
  mutate(type = "Relative") %>%
  bind_rows(
    abs_fits %>%
    group_by(model, error_type,  train) %>%
    summarise(RMSE = nrmse_func(truth = expected, estimate = calibrated_abs, type = "sd")) %>%
    mutate(type = "Absolute")) %>%
  dplyr::filter(error_type %in% c("error_prop", "error_abs", "error_clr", "error_alr")) %>%
  mutate(
    error_type = error_type %>% str_remove("error_"),
    model = factor(model, levels = c(
      "XGBOOST Tree",
      "Random Forest",
      "SVM",
      "LASSO Regression",
      "Linear Regression",
      "metacal", 
      "Uncorrected")),
    train = factor(train, levels=c("Training set", "Test set")),
    type = factor(type, levels = c("Relative", "Absolute"))) %>%
  ggplot(aes(x = model, y=RMSE, fill=error_type))+
  geom_point(position=position_dodge(0.5), shape=21, size=2, colour="black")+
  scale_fill_viridis_d()+
  #scale_fill_brewer(palette="Paired")+
  facet_grid(train~type)+ 
  coord_flip() +
  base_theme +
  theme(legend.position = "bottom")+
  labs(y = "Normalised RMSE",
       x = "Model Type",
       fill = "Transformation")

gg.rmse


# Corrections model
gg.cal <- fits %>%
  mutate(type = "Relative",
             train = factor(train, levels=c("Training set", "Test set"))) %>%
  filter(error_type == "error_abs", model %in% c("Random Forest", "Uncorrected")) %>%
  mutate(paired = paste0(sample_id, taxon)) %>%
  ggplot(aes(calibrated, expected.prop, fill = model, group = paired)) +
  geom_abline(lty = 2, colour = "gray80", size = 1) +
  geom_line(colour="gray", alpha=0.5, position = ggstance::position_dodgev(height=0.1)) +
  geom_point(alpha = 0.5, shape=21, colour="black", position = ggstance::position_dodgev(height=0.1)) +
  base_theme+
  facet_grid(train~type)+
  scale_x_continuous(trans = scales::pseudo_log_trans(1e-3), labels=scales::percent,
                       breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_y_continuous(trans = scales::pseudo_log_trans(1e-3),
                     labels=scales::percent, breaks=c(0, 0.025, 0.05, 0.1, 0.25, 0.5, 1), limits=c(0,1))+
  scale_fill_manual(values=c("red", "gray"))+
    theme(panel.grid = element_line(size = rel(1)),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "bottom",
        legend.text = element_text(face="italic")) +
    labs(x = "Calibrated proportions", 
        y = "Expected proportions",
        fill = "Sample type", 
        colour= NULL)


gg.cal_abs <- abs_fits %>%
  mutate(type = "Absolute",
             train = factor(train, levels=c("Training set", "Test set"))) %>%
  filter(error_type == "error_abs", model %in% c("Random Forest", "Uncorrected")) %>%
  mutate(paired = paste0(sample_id, taxon)) %>%
  ggplot(aes(calibrated_abs, expected, fill = model, group = paired)) +
  geom_abline(lty = 2, colour = "gray80", size = 1) +
  geom_line(colour="gray", alpha=0.5, position = ggstance::position_dodgev(height=1)) +
  geom_point(alpha = 0.5, shape=21, colour="black", position = ggstance::position_dodgev(height=1)) +
  base_theme+
  facet_grid(train~type)+
  scale_x_continuous(trans = scales::pseudo_log_trans(1), breaks=c(0, 5, 10, 25, 50, 100, 200, 300), limits=c(0,340))+
  scale_y_continuous(trans = scales::pseudo_log_trans(1), breaks=c(0, 5, 10, 25, 50, 100, 200, 300), limits=c(0,340))+
  scale_fill_manual(values=c("red", "gray"))+
    theme(panel.grid = element_line(size = rel(1)),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "none",
        legend.text = element_text(face="italic")) +
    labs(x = "Calibrated Individuals", 
        y = "Expected Individuals",
        fill = "Sample type", 
        colour= NULL)

gg.cal_abs

 
gg.rmse - (gg.cal - gg.cal_abs)
```


# Fit final model

```{r Final model}
final_fit <- joint_train %>%
  filter(error_type == "error_abs") %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(train=data)%>%
  left_join(joint_test %>%
  filter(expected > 0) %>%
  group_by(error_type) %>%
  nest() %>%
  dplyr::rename(test=data)) %>%
  left_join(group_keys(.) %>%
            tidyr::unite("group_names", everything(), sep="-", remove=FALSE)) %>%
  mutate(
    model_obj = purrr::map(train, ~safe_fit(ranger_workflow_prop, data=.x)),
    pred_train = purrr::map2(model_obj, train, ~safe_predict(.x, .y)),
    pred_test = purrr::map2(model_obj, test, ~safe_predict(.x, .y)),
    vip = purrr::map2(model_obj, group_names,~{
      if(!is(.x, "data.frame")){
        .x %>%
        pull_workflow_fit() %>%
        vip::vip()+
        labs(title=.y %>% str_remove("error_"))
      } else{
        return(NULL)
      }
      
    })
  )

# Plot model variable importance
wrap_plots(final_fit$vip)

#results
final_results_train <- final_fit %>%
  unnest(pred_train, train) %>% 
  dplyr::select(!where(is.list))%>%
  mutate(
  estimated  = expit(.pred),
  observed.prop = expit(observed.prop),
  expected.prop = expit(expected.prop),
  predicted = estimated,
  calibrated = estimated,
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

final_results_train %>%
  #group_by(material_type, fcid) %>%
  rmse(truth = observed.prop, estimate = predicted)

# Results test set
final_results_test <- final_fit %>%
  unnest(pred_test, test) %>% 
  dplyr::select(!where(is.list))%>%
  mutate(
  estimated  = expit(.pred),
  observed.prop = expit(observed.prop),
  expected.prop = expit(expected.prop),
  predicted = estimated,
  calibrated = estimated,
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

final_results_test %>%
  group_by(material_type, fcid) %>%
  rmse(truth = observed.prop, estimate = predicted)

# See how resamples looks
ranger_workflow_prop %>%
  fit_resamples(train_cv) %>%
  unnest(.metrics) %>%
  filter(.metric == "rmse") %>%
  summarise(rmse = mean(.estimate))

```


# How does the final calibration actually look?

```{r final bars}
# Calibrated RA
gg.calbars_ra <- final_results_test  %>%
    dplyr::rename(Actual = expected.prop,
                  Observed = observed.prop,
                  Calibrated = calibrated) %>%
    pivot_longer(c("Actual", "Observed", "Calibrated"),
                 names_to = "type",
                 values_to = "abundance") %>%
    mutate(type = factor(type, c("Actual","Observed", "Calibrated"))) %>%
  ggplot(aes(type, abundance, fill = taxon)) +
    geom_col() +
    facet_wrap(~sample_id, drop=TRUE) +
    base_theme +
    theme(legend.position = "right",
          legend.text = element_text(face="italic"))+
    scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(labels = scales::percent)+
  labs(title="Final RF model",
       x = NULL,
       y = "Relative Abundance")

# Calibrated abs
gg.calbars_abs <- final_results_test  %>%
    mutate(calibrated = calibrated * sample_size) %>%
    mutate(taxon = taxon %>% str_replace("_", " ")) %>%
    dplyr::rename(Actual = expected,
                  Observed = observed_abs,
                  Calibrated = calibrated) %>%
    pivot_longer(c("Actual", "Observed", "Calibrated"),
                 names_to = "type",
                 values_to = "abundance") %>%
    mutate(type = factor(type, c("Actual","Observed", "Calibrated"))) %>%
  ggplot(aes(type, abundance, fill = taxon)) +
    geom_col() +
    facet_wrap(~sample_id, drop=TRUE) +
    base_theme +
    theme(legend.position = "right",
          legend.text = element_text(face="italic"))+
    scale_fill_brewer(palette = "Paired") +
  labs(title="Final RF model",
       x = NULL,
       y = "Total individuals",
       fill = "Taxon")

#Save figure
pdf(file="fig/calibrated_bars.pdf", width = 11, height = 8 , paper="a4r")
  plot(gg.calbars_ra)
  plot(gg.calbars_abs)
try(dev.off(), silent=TRUE)
  
```

# Jacknife RMSE estimates

```{r jacknife}
ranger_workflow_prop2 <- 
  workflow() %>% 
  add_recipe(prop_recipe) %>% 
  add_model(ranger_spec) 

ranger_workflow_prop2 <- finalize_workflow(ranger_workflow_prop2, tibble(
  mtry=5, min_n=6
))

# Bootstrapped Standard errors with number of control samples
samples <- unique(joint_train$sample_id)

boot_df <- joint_train %>%
  filter(error_type == "error_prop") %>%
  filter(expected > 0)

jacknifes <- seq(1,length(samples),1) %>%
  purrr::map_df(function(x){
    boots <- rep(x, 100) #How many resamples at each size? 
    names(boots) <- paste0("boot.",seq(1,length(boots),1))
    out <- purrr::map2_df(boots, names(boots), function(y,z){
        subsamp <- sample(samples, y, replace = TRUE)
        boot_df %>% 
          filter(sample_id %in% subsamp) %>%
          mutate(id = z,
                 size=y)
      })
  })%>%
  group_by(id,size) %>%
  nest() %>%
  dplyr::rename(train = data) %>%
  mutate(test = joint_test %>%
    filter(error_type == "error_prop") %>%
      nest(everything()))

# using safe predict function to stop it failing when variables arent available
fit_jacknifes <- jacknifes %>%
  filter(size > 3) %>% 
  ungroup() %>%
  mutate(
    model_obj = purrr::map(train, ~safe_fit(ranger_workflow_prop2, data=.x)),
    pred_test = purrr::map2(model_obj, test$data, ~safe_predict(.x, .y))
    ) %>%
  filter(!map_lgl(pred_test, is.null)) %>%
  group_by(size) %>%
  mutate(n_boots = n())

#saveRDS(fit_jacknifes, "output/jacknifes.rds")

jacknife_results <- fit_jacknifes %>%
  unnest(pred_test, test$data)%>%
  dplyr::select(!where(is.list))%>%
  mutate(
  estimated  = expit(.pred),
  observed.prop = expit(observed.prop),
  expected.prop = expit(expected.prop),
  predicted = estimated,
  calibrated = estimated,
  residual = observed.prop / predicted
  ) %>%
  group_by(sample_id, error_type, id, size) %>%
  mutate_at(vars(predicted, calibrated, observed.prop, expected.prop), ~ . / sum(., na.rm=TRUE) ) %>%
  ungroup() %>%
  dplyr::filter(expected > 0)

gg.jacknife_box <- jacknife_results %>%
  group_by(size, id, material_type) %>%
  rmse(truth = observed.prop, estimate = predicted) %>%
  ggplot(aes(x = size, y = .estimate, group=size, fill=material_type,  colour=material_type)) +
  geom_point(alpha=0.2, position = position_jitter(width=0.2))+
  geom_boxplot(alpha=1, outlier.colour = NA, fill=NA, colour="black") +
  scale_fill_brewer(palette="Set1") +
  scale_colour_brewer(palette="Set1") +
  base_theme +
  facet_grid(material_type~.)+
  labs(x = "Number of training samples",
       y = "RMSE (Test set)")


jacknife_quantiles <- jacknife_results %>%
  group_by(size, id, material_type) %>%
  rmse(truth = observed.prop, estimate = predicted) %>%
  group_by(size, material_type) %>%
  group_modify(~{
    stats <- .x %>%
      pull(.estimate)
    alpha=0.05
    ci <- stats %>% quantile(probs = c(alpha/2, 1 - alpha/2), na.rm = TRUE)
    res <- tibble(.lower = min(ci), .estimate = mean(stats, na.rm = TRUE), 
        .upper = max(ci), .alpha = alpha, .method = "percentile")
    return(res)
  })

gg.jacknife_ribbon <- jacknife_quantiles %>%
  ggplot(aes(x = size, y = .estimate, fill=material_type, colour=material_type)) +
  geom_line() + 
  geom_ribbon(aes(ymax = .upper, ymin=.lower), alpha=0.1, colour=NA) +
  scale_fill_brewer(palette="Set1") +
  scale_colour_brewer(palette="Set1") +
  base_theme +
  facet_grid(material_type~.)+
  labs(x = "Number of training samples",
       y = "RMSE (Test set)")
  
#Save figure
pdf(file="fig/jacknife_rmse.pdf", width = 11, height = 8 , paper="a4r")
  plot(gg.jacknife_box)
  plot(gg.jacknife_ribbon)
try(dev.off(), silent=TRUE)

```
